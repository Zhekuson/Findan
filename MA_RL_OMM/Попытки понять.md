## Что за статья 

в HFT не особо изучен RL  
Проблемы классического мл:
1) время предсказания (долго)
2) рынки - это рандом
3) Policy optimization - а как? надо как-то принимать решения в ситуациях по типу 45/55 и обычно это делает человек. А если меняется ситуация на рынке, то политика оптимизации зачастую не может адаптироваться


### Что делаем
Macro-agent: На макро-уровне (тик-минута) принимает свои решения
Micro-agent: имеет больше данных (каждый микро-тик), может делать сделки каждые 10 секунд

#### Данные
Взяли подробную инфу с помощью подписки на bittrex 2-17 ноября 2018 года
взяли курс биткоина, и за эти дни сделали типа historical orderbook  
In total, there were
41830629 trade, bid, and ask data points, and 10945 minute
tick data points. However, the entire dataset was not
used. Instead, only the most recent few days were chosen
for this study.

### RL
Reinforcement learning is a learning technique in machine
learning, in particular sequential decision making, where an
agent learns to take actions optimally in an environment.
Unlike supervised learning, in the context of a reinforcement
learning problem, this agent learns to take actions that maximize
the reward it receives from the environment. At each
time-step, the agent observes its environment, and takes an
action based on the observation. The environment provides
feedback on how well the action performed in the form of
a reward.

In the case of this problem, these rewards can be
delayed. An instant reward feedback for an action to buy, sell,
or hold, or placing a limit or market order at the current timestep
may not be given. For example, the decision to hold an
asset may not yield an instant reward.

Формализовали в данном случае в виде **Markov Decision Process**  
(По аналогии с марковскими цепями):

*the effects of some action taken in some state depends only
on that state and not on prior states encountered*

Судя по траблам (вторая проблема), интуитивно кажется, что процесс Марковский, потому что зависит только от предыдущего шага

S - состояния, A - действия, P - распределение вероятностей перехода
$P: S\times A \times S\mapsto \reals$

r : S -> R  
$r: S\mapsto \reals$
reward function

$\gamma \in (0; 1)$ is the discount factor which determines the
importance of future rewards.

На шаге $t$ будущая награда с дискаунтом  
$R_t = \sum_{i=t}^{T}{\gamma^{i-t}\cdot r_i}$  
T - последний шаг, потенциально - бесконечность

В случае маркет-мейкинга состояние более сложно, чем кажется.
Состояние полностью неизвестно, мы не знаем, сколько трейдеров сейчас на рынке

Therefore, instead of an MDP, this is a **Partially Observable
Markov Decision Process** (POMDP)

$S^{'} \sim\mathcal{O}(S)$   
Nevertheless, given a proper simulation of
the environment, the agent should be able to optimize well
against these unknowns.

У нас continuous time-steps for the reason that continuous timesteps
would not be possible in the real world since the
web-socket data arrives at discrete time-steps  

Особый вариант - **deep Q-learning**  

$\pi:S \mapsto A$ - ищем оптимальную такую функции политики
