## Что за статья 

в HFT не особо изучен RL  
Проблемы классического мл:
1) время предсказания (долго)
2) рынки - это рандом
3) Policy optimization - а как? надо как-то принимать решения в ситуациях по типу 45/55 и обычно это делает человек. А если меняется ситуация на рынке, то политика оптимизации зачастую не может адаптироваться


### Что делаем
Macro-agent: На макро-уровне (тик-минута) принимает свои решения
Micro-agent: имеет больше данных (каждый микро-тик), может делать сделки каждые 10 секунд

#### Данные
Взяли подробную инфу с помощью подписки на bittrex 2-17 ноября 2018 года
взяли курс биткоина, и за эти дни сделали типа historical orderbook  
In total, there were
41830629 trade, bid, and ask data points, and 10945 minute
tick data points. However, the entire dataset was not
used. Instead, only the most recent few days were chosen
for this study.

### RL
Reinforcement learning is a learning technique in machine
learning, in particular sequential decision making, where an
agent learns to take actions optimally in an environment.
Unlike supervised learning, in the context of a reinforcement
learning problem, this agent learns to take actions that maximize
the reward it receives from the environment. At each
time-step, the agent observes its environment, and takes an
action based on the observation. The environment provides
feedback on how well the action performed in the form of
a reward.

In the case of this problem, these rewards can be
delayed. An instant reward feedback for an action to buy, sell,
or hold, or placing a limit or market order at the current timestep
may not be given. For example, the decision to hold an
asset may not yield an instant reward.

Формализовали в данном случае в виде **Markov Decision Process**  
(По аналогии с марковскими цепями):

*the effects of some action taken in some state depends only
on that state and not on prior states encountered*

Судя по траблам (вторая проблема), интуитивно кажется, что процесс Марковский, потому что зависит только от предыдущего шага

S - состояния, A - действия, P - распределение вероятностей перехода
$P: S\times A \times S\mapsto \reals$

r : S -> R  
$r: S\mapsto \reals$
reward function

$\gamma \in (0; 1)$ is the discount factor which determines the
importance of future rewards.

На шаге $t$ будущая награда с дискаунтом  
$R_t = \sum_{i=t}^{T}{\gamma^{i-t}\cdot r_i}$  
T - последний шаг, потенциально - бесконечность

В случае маркет-мейкинга состояние более сложно, чем кажется.
Состояние полностью неизвестно, мы не знаем, сколько трейдеров сейчас на рынке

Therefore, instead of an MDP, this is a **Partially Observable
Markov Decision Process** (POMDP)

$S^{'} \sim\mathcal{O}(S)$   
Nevertheless, given a proper simulation of
the environment, the agent should be able to optimize well
against these unknowns.

У нас continuous time-steps for the reason that continuous timesteps
would not be possible in the real world since the
web-socket data arrives at discrete time-steps  

Особый вариант - **deep Q-learning**  

$\pi:S \mapsto A$ - ищем оптимальную такую функции политики

Q-learning is
a model-free value-based approach, which uses the action-value
function **Q(s; a)**

$Q^{\pi}(s,a) = \mathbb{E}_{\pi}[R_t| S_t = s,A_t=a, \pi]$  
(Условное матожидание)  
Матожидание награды при условии, что состояние **s**, а действие **a**

$Q_{t+1}(s_t, a_t) = (1-\alpha)\cdot Q_{t}(s_t, a_t) + \alpha(r_{t+1} + \gamma  \underset{a}{max}Q_t(s_{t+1}, a))$  


However, in the case of this problem, a neural network is
used to approximate the Q-value function, where the input
is the state (instead of state-action pairs in Q-learning), and
the output are the Q-values for the various actions

Thus, the
Q-value function is parameterized by weights $\theta$, то есть:   
$Q(s,a;\theta) \approx Q^*(s,a)$  
В RL трейд-офф между "эксплоит" и "эксплор" (используй то, что изучил vs изучай)  

Берем с гасящимся весом эпсилон-жадный подход  
с вероятносью $\epsilon$ делаем случайное действие (1-$\epsilon$ - не случайное) и эта величина уменьшается со временем, чтобы в конце больше полагаться на опыт

Еще одна вещь - **experience replay**  
By storing the agent’s experiences
in a memory data structure, and having the agent relive those
randomly sampled batches of experiences often, it helps to
alleviate the high temporal correlation in the data  

А еще, когда учишься на этих случайных батчах, создается ощущение тренировок на независимых одинаково распределенных случайных величин, а еще стохастический градиентный спуск на этом работает

## Macro agent  
The Macro-agent is responsible for making a discrete
decision to buy, hold, or sell an asset by looking at the data
from a macro level  

Весь датасет не использовали, потому что агент мог недоизучить последние состояния и переизучить другие

Большая часть данных менее волатильна, но последние дни она была высокой, агент бы не справился, потому что привык к низкой волатильности
  
course of the last few days is used: November 15th, 2018 to
November 17th, 2018  

To train the agent, the dataset is split into training and
test sets. The training set consists of the time-steps prior to
November 16th, 2018.

## State

The state space at a certain time-step **t** consists of historical
price data in the range **t-h** to **t**, where h denotes how
far back in history the agent looks.

А еще индикаторы, **z-score**

$z_x = \frac{x-\mu}{\sigma}$

его можно расширить через окно в **n** шагов  
$p_t$ - цена закрытия на шаге t, SMA($p_{t-n, t}$) - simple moving average за **n** шагов до t, аналогично STDDEV, по итогу получается z-score в окне

$z^n_t = \frac{p_t - SMA(p_{t-n,t})}{STDDEV(p_{t-n,t})}$   

Вытащенные фичи: 
1) *Price level* - ниче не понял, походу чистый zscore для цен
2) *Price change*: Текущая цена сравнивается со средним до этого и потом берется zscore от этого   
   $PC^n_t = \frac{p_t}{SMA(p_{t-n,t})} - 1$
3) *Volume Level* - zscore для объемов?  
4) *Volume Change* - по аналогии с Price change и берем также zscore
    $VC^n_t = \frac{v_t}{SMA(v_{t-n,t})} - 1$  
5) *Volatility*: Берем EMA(Exponential) вместо SMA (потому что дает больше вес последним данным)  
   $EMA^{n}_{t} = \frac{2p_t}{n+1} + \frac{\sum^{t}_{j=t-n}{p_j}}{n}(100 - \frac{2}{n+1})$    
    Итого волатильность за последние **m** дней:  
   
   Volatility$^m_t = \frac{EMA_{t,n} - EMA_{t-m,n}}{EMA_{t-m,n}}$  

6) current assets (цена, за которую агент их купил)

## Action

1) **Buy** - берем по текущей цене (открытия) один биткоин, добавляем в ассеты цену (типа купили)
2) **Sell** - продаем ВСЕ по текущей цене открытия
3) **Hold** - ничего не делаем

## Reward

After the agent takes an action, the environment outputs a
reward for that action. Algorithm 1 depicts how rewards are
handled

Algorithm 1: Macro-agent Reward Function  

> if action == HOLD then  
>> reward = 0
>
> else if action == BUY then  
>> Append current open price to current assets list  
>> reward = 0  
>
>else if action == SELL then  
>> if there are no assets to sell then  
>>> reward = -1  
>>
>> else  
>>> reward  = sell off all assets from current assets list and determine profit based on current open price
> end
>
> Clip Rewards  

Обрезаем до -1, 0 или 1

## Deep-Q

The Q-network takes in
as inputs the price data, market indicator data and the current
assets list, and outputs 3 Q-values which are then used to
determine the optimal action.  
Использовали Adam 
Каждую эпоху на всем тренировочном сете

Дальше тут описание алгоритма для макроагента, я пошел реализовывать  
А нет, надо сначала данные достать
