{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from macro_agent import MacroAgent, ReplayMemory, QNetwork, Transition\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "#from mpl_finance import candlestick_ohlc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import datetime as datetime\n",
    "from datetime import datetime\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPACITY = 500\n",
    "INIT_EPSILON = 0.9\n",
    "GAMMA_DISCOUNT = 0.8\n",
    "EPOCH_COUNT = 500\n",
    "MINI_BATCH_SIZE = 10\n",
    "WINDOW_SIZE = 20\n",
    "ACTIONS={\"HOLD\":0, \"SELL\":1, \"BUY\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self, all_data):\n",
    "        self._all_data = all_data\n",
    "        self._pointer = 0\n",
    "        self.done = False\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def taken_action(self):\n",
    "        self._pointer += 1\n",
    "        if self._pointer >= len(self._all_data) - 1:\n",
    "            self.done = True\n",
    "        pass\n",
    "    \n",
    "    def get_price(self):\n",
    "        return self._all_data.iloc[self._pointer]['Close']\n",
    "\n",
    "    def get_change_zscore(self, column):\n",
    "        start_idx = np.max([0, self._pointer - WINDOW_SIZE])\n",
    "        frame = self._all_data.iloc[start_idx:self._pointer + 1]\n",
    "        market_now = self._all_data.iloc[self._pointer]\n",
    "        PC = market_now[column] / np.mean(frame[column]) - 1\n",
    "\n",
    "        PCs = np.empty_like(frame[column])\n",
    "        for i in range(start_idx, self._pointer + 1):\n",
    "            start_frame = np.max([0, i - WINDOW_SIZE])\n",
    "            end_frame = i\n",
    "            PCs[i - start_idx] = self._all_data.iloc[end_frame][column] / np.mean(\n",
    "                self._all_data.iloc[start_frame:end_frame+1][column]) - 1\n",
    "\n",
    "        z_score_price_change = (PC - np.mean(PCs)) / np.std(PCs) \n",
    "        return z_score_price_change\n",
    "\n",
    "    def get_EMA(self, t):\n",
    "        start_idx = np.max([0, self._pointer - WINDOW_SIZE])\n",
    "        g = 2 * self._all_data.iloc[t]['Close'] / (WINDOW_SIZE + 1)\n",
    "        ex = (100 - 2/(WINDOW_SIZE + 1))\n",
    "        EMA = g + np.mean(self._all_data.iloc[start_idx:t+1]['Close']) * ex\n",
    "        return EMA\n",
    "        \n",
    "\n",
    "    def get_indicators(self):\n",
    "        start_idx = np.max([0, self._pointer - WINDOW_SIZE])\n",
    "        market_now = self._all_data.iloc[self._pointer]\n",
    "        frame = self._all_data.iloc[start_idx:self._pointer + 1]\n",
    "        # price\n",
    "        z_score_price = (market_now['Close'] - \n",
    "            np.mean(frame['Close'])) / np.std(frame['Close'])\n",
    "        # price change\n",
    "        z_score_price_change = self.get_change_zscore('Close')\n",
    "        # volume\n",
    "        z_score_volume = (market_now['Volume'] - \n",
    "            np.mean(frame['Volume'])) / np.std(frame['Volume'])\n",
    "        # volume change\n",
    "        z_score_volume_change = self.get_change_zscore('Volume')\n",
    "        # Volatility\n",
    "        volatility = (self.get_EMA(self._pointer) -\n",
    "         self.get_EMA(self._pointer - WINDOW_SIZE))/ self.get_EMA(self._pointer - WINDOW_SIZE)\n",
    "        return z_score_price,z_score_price_change,z_score_volume,z_score_volume_change,volatility\n",
    "\n",
    "    def get_state(self, agent:MacroAgent):\n",
    "        return torch.tensor(np.hstack((self.get_price(), self.get_indicators(),\n",
    "            agent.estimate_assets(self.get_price()))), dtype=torch.float32)\n",
    "\n",
    "def decay_epsilon(cur_epsilon):\n",
    "    return cur_epsilon * 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    market_data = pd.read_json('RESULT.json')\n",
    "    market_data.rename(columns={1:'Open',2:'High', 3:'Low', 4:'Close', 5:'Volume'}, inplace=True)\n",
    "    market_data[0] = market_data[0].transform(datetime.fromtimestamp)\n",
    "    market_data.set_index([0], inplace=True)\n",
    "    market_data.sort_index(inplace=True)\n",
    "    market_data = market_data[(market_data.index >= '2018-11-15 00:00:00') & (market_data.index <= '2018-11-17 17:06:00')]\n",
    "    return market_data\n",
    "\n",
    "def get_train_data(market_data):\n",
    "    return market_data[market_data.index <= '2018-11-16 00:00:00']\n",
    "\n",
    "def get_test_data(market_data):\n",
    "    return market_data[market_data.index >= '2018-11-16 00:00:00']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = ReplayMemory(CAPACITY)\n",
    "macro_agent = MacroAgent()\n",
    "optimizer = torch.optim.Adam(macro_agent.q_network.parameters())\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_algo(agent:MacroAgent, action, environment):\n",
    "    if action == ACTIONS['SELL']:\n",
    "        if len(agent.assets) == 0:\n",
    "            cur_reward = -1\n",
    "        else:\n",
    "            earning = agent.sell_assets(environment.get_price())\n",
    "            cur_reward = 1 if earning > 0 else -1\n",
    "            \n",
    "    elif action == ACTIONS['BUY']:\n",
    "        agent.buy_asset(environment.get_price())\n",
    "        cur_reward = 0\n",
    "    else:\n",
    "        cur_reward = 0\n",
    "    return cur_reward\n",
    "\n",
    "def reward_algo_total(taken_actions, agent:MacroAgent, environment):\n",
    "    results = torch.empty_like(taken_actions, dtype=torch.float32)\n",
    "    for idx in range(len(taken_actions)):\n",
    "        results[idx] = reward_algo(agent, taken_actions[idx], environment)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5200, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5200, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5200, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5200, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:08<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20012/1375162824.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# grad d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mmacro_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mcurrent_states_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_states_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         output_actions = torch.tensor(torch.argmax(macro_agent.q_network.forward(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "market_data = load_data()\n",
    "train_data = get_train_data(market_data)\n",
    "test_data = get_test_data(market_data)\n",
    "current_max_balance = 0\n",
    "for epoch in tqdm(range(EPOCH_COUNT)):\n",
    "    done = False\n",
    "    cur_epsilon = INIT_EPSILON\n",
    "    environment = Environment(train_data)\n",
    "    macro_agent.sell_assets(0)\n",
    "    while not done:\n",
    "    \n",
    "        now_state = environment.get_state(macro_agent)\n",
    "        macro_agent.q_network.eval()\n",
    "        decision = np.random.rand()\n",
    "        if decision < cur_epsilon:\n",
    "            # epsilon\n",
    "            action = np.random.choice(3)\n",
    "        else:\n",
    "            # 1-epsilon\n",
    "            action = torch.argmax(macro_agent.q_network(now_state))\n",
    "        \n",
    "\n",
    "        # reward algo\n",
    "        cur_reward = reward_algo(macro_agent, action, environment)\n",
    "\n",
    "        environment.taken_action()\n",
    "        next_state = environment.get_state(macro_agent)\n",
    "        done = environment.done\n",
    "        \n",
    "\n",
    "        replay_memory.push(now_state, action, cur_reward, next_state, done)\n",
    "        \n",
    "        # taking batch\n",
    "        batch = replay_memory.sample(MINI_BATCH_SIZE)\n",
    "        q = np.empty(len(batch))\n",
    "        current_states_batch = []\n",
    "\n",
    "        for i, object in enumerate(batch):\n",
    "            current_states_batch.append(torch.tensor(object.state, dtype=torch.float32))\n",
    "            if not object.done:\n",
    "                # r_i + gamma * Q()\n",
    "                cur_in = torch.tensor(\n",
    "                    object.next_state, dtype=torch.float32).unsqueeze(0)\n",
    "                act = torch.argmax(macro_agent.q_network.forward(cur_in))\n",
    "                q[i] = object.reward + GAMMA_DISCOUNT * reward_algo(macro_agent, act, environment)\n",
    "            else:\n",
    "                # r_i\n",
    "                q[i] = object.reward\n",
    "        # grad d\n",
    "        macro_agent.q_network.train()\n",
    "        current_states_batch = torch.stack(current_states_batch)\n",
    "        \n",
    "        output_actions = torch.tensor(torch.argmax(macro_agent.q_network.forward(\n",
    "            current_states_batch), dim=1), dtype=torch.float32)\n",
    "\n",
    "        rewards = reward_algo_total(output_actions, macro_agent, environment)\n",
    "        #print(rewards, q)\n",
    "        loss = criterion(torch.tensor(q, dtype=torch.float32, requires_grad=True), \n",
    "            rewards)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #optimizer.zero_grad()\n",
    "\n",
    "        cur_epsilon = decay_epsilon(cur_epsilon)  \n",
    "    \n",
    "\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        # test model\n",
    "        macro_agent.q_network.eval()\n",
    "        test_env = Environment(test_data)\n",
    "        macro_agent.sell_assets(price=0)\n",
    "        current_balance = [0]\n",
    "        while not test_env.done:\n",
    "            now_state = test_env.get_state(macro_agent)\n",
    "            action = torch.argmax(macro_agent.q_network(now_state)).float()\n",
    "            price = test_env.get_price()\n",
    "            if action == ACTIONS['BUY']:\n",
    "                prev_assets = macro_agent.estimate_assets(price)\n",
    "                macro_agent.buy_asset(price)\n",
    "                now_assets = macro_agent.estimate_assets(price) \n",
    "                current_balance.append(current_balance[-1] + now_assets - prev_assets - price)\n",
    "            elif action == ACTIONS['SELL']:\n",
    "                earning = macro_agent.sell_assets(price)\n",
    "                current_balance.append(earning)\n",
    "            else:\n",
    "                current_balance.append(macro_agent.estimate_assets(price))\n",
    "            test_env.taken_action()\n",
    "        plt.plot(current_balance)\n",
    "        pd.DataFrame(current_balance).to_csv('macro_agent_epoch_{}.csv'.format(epoch))\n",
    "        plt.savefig('testing_epoch_{}.jpg'.format(epoch))\n",
    "\n",
    "        if current_balance[-1] > current_max_balance:\n",
    "            torch.save(macro_agent.q_network.state_dict(), 'best_q_net_epoch_{}'.format(epoch))\n",
    "\n",
    "    torch.save(macro_agent.q_network.state_dict(), 'q_net_epoch_{}'.format(epoch))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3746, -0.3235,  0.0076,  0.0131,  0.1609, -0.2423, -0.3071],\n",
       "        [ 0.0325, -0.3361, -0.0678,  0.1092,  0.2856, -0.0894, -0.0896],\n",
       "        [ 0.2877,  0.1441,  0.0769,  0.0705,  0.2919,  0.1385,  0.2284],\n",
       "        [-0.0973,  0.1532,  0.0725, -0.0172, -0.1954,  0.2231,  0.2433],\n",
       "        [ 0.3384, -0.2137, -0.1595,  0.0083, -0.1695, -0.0801, -0.3170],\n",
       "        [ 0.2473,  0.0634,  0.0417, -0.1358, -0.2976,  0.1134,  0.1790],\n",
       "        [-0.0425,  0.3571, -0.0076, -0.0049,  0.0072, -0.1257, -0.1256]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b2bd16c4a5b7bdb943c513e9c5dbb440fb1e7bca9cf829d440576f5014c581f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
