{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from macro_agent import MacroAgent, ReplayMemory, QNetwork, Transition\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "#from mpl_finance import candlestick_ohlc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import datetime as datetime\n",
    "from datetime import datetime\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPACITY = 500\n",
    "INIT_EPSILON = 0.9\n",
    "GAMMA_DISCOUNT = 0.8\n",
    "EPOCH_COUNT = 500\n",
    "MINI_BATCH_SIZE = 10\n",
    "WINDOW_SIZE = 20\n",
    "ACTIONS={\"HOLD\":0, \"SELL\":1, \"BUY\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self, all_data):\n",
    "        self._all_data = all_data\n",
    "        self._pointer = 0\n",
    "        self.done = False\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def taken_action(self):\n",
    "        self._pointer += 1\n",
    "        if self._pointer >= len(self._all_data) - 1:\n",
    "            self.done = True\n",
    "        pass\n",
    "    \n",
    "    def get_price(self):\n",
    "        return self._all_data.iloc[self._pointer]['Close']\n",
    "\n",
    "    def get_change_zscore(self, column):\n",
    "        start_idx = np.max([0, self._pointer - WINDOW_SIZE])\n",
    "        frame = self._all_data.iloc[start_idx:self._pointer + 1]\n",
    "        market_now = self._all_data.iloc[self._pointer]\n",
    "        PC = market_now[column] / np.mean(frame[column]) - 1\n",
    "\n",
    "        PCs = np.empty_like(frame[column])\n",
    "        for i in range(start_idx, self._pointer + 1):\n",
    "            start_frame = np.max([0, i - WINDOW_SIZE])\n",
    "            end_frame = i\n",
    "            PCs[i - start_idx] = self._all_data.iloc[end_frame][column] / np.mean(\n",
    "                self._all_data.iloc[start_frame:end_frame+1][column]) - 1\n",
    "\n",
    "        z_score_price_change = (PC - np.mean(PCs)) / np.std(PCs) \n",
    "        return z_score_price_change\n",
    "\n",
    "    def get_EMA(self, t):\n",
    "        start_idx = np.max([0, self._pointer - WINDOW_SIZE])\n",
    "        g = 2 * self._all_data.iloc[t]['Close'] / (WINDOW_SIZE + 1)\n",
    "        ex = (100 - 2/(WINDOW_SIZE + 1))\n",
    "        EMA = g + np.mean(self._all_data.iloc[start_idx:t+1]['Close']) * ex\n",
    "        return EMA\n",
    "        \n",
    "\n",
    "    def get_indicators(self):\n",
    "        start_idx = np.max([0, self._pointer - WINDOW_SIZE])\n",
    "        market_now = self._all_data.iloc[self._pointer]\n",
    "        frame = self._all_data.iloc[start_idx:self._pointer + 1]\n",
    "        # price\n",
    "        z_score_price = (market_now['Close'] - \n",
    "            np.mean(frame['Close'])) / np.std(frame['Close'])\n",
    "        # price change\n",
    "        z_score_price_change = self.get_change_zscore('Close')\n",
    "        # volume\n",
    "        z_score_volume = (market_now['Volume'] - \n",
    "            np.mean(frame['Volume'])) / np.std(frame['Volume'])\n",
    "        # volume change\n",
    "        z_score_volume_change = self.get_change_zscore('Volume')\n",
    "        # Volatility\n",
    "        volatility = (self.get_EMA(self._pointer) -\n",
    "         self.get_EMA(self._pointer - WINDOW_SIZE))/ self.get_EMA(self._pointer - WINDOW_SIZE)\n",
    "        return z_score_price,z_score_price_change,z_score_volume,z_score_volume_change,volatility\n",
    "\n",
    "def decay_epsilon(cur_epsilon):\n",
    "    return cur_epsilon * 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    market_data = pd.read_json('RESULT.json')\n",
    "    market_data.rename(columns={1:'Open',2:'High', 3:'Low', 4:'Close', 5:'Volume'}, inplace=True)\n",
    "    market_data[0] = market_data[0].transform(datetime.fromtimestamp)\n",
    "    market_data.set_index([0], inplace=True)\n",
    "    market_data.sort_index(inplace=True)\n",
    "    market_data = market_data[(market_data.index >= '2018-11-15 00:00:00') & (market_data.index <= '2018-11-17 17:06:00')]\n",
    "    return market_data\n",
    "\n",
    "def get_train_data(market_data):\n",
    "    return market_data[market_data.index <= '2018-11-16 00:00:00']\n",
    "\n",
    "def get_test_data(market_data):\n",
    "    return market_data[market_data.index >= '2018-11-16 00:00:00']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = ReplayMemory(CAPACITY)\n",
    "macro_agent = MacroAgent()\n",
    "optimizer = torch.optim.Adam(macro_agent.q_network.parameters())\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_algo(agent:MacroAgent, action, environment):\n",
    "    if action == ACTIONS['SELL']:\n",
    "        if len(agent.assets) == 0:\n",
    "            cur_reward = -1\n",
    "        else:\n",
    "            earning = agent.sell_assets(environment.get_price())\n",
    "            cur_reward = 1 if earning > 0 else -1\n",
    "            \n",
    "    elif action == ACTIONS['BUY']:\n",
    "        agent.buy_asset(environment.get_price())\n",
    "        cur_reward = 0\n",
    "    else:\n",
    "        cur_reward = 0\n",
    "    return cur_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected 3D input (got 1D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27168/487403219.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                 \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmacro_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m                 \u001b[0mq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mGAMMA_DISCOUNT\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mreward_algo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmacro_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Zhekuson\\Documents\\Findan\\Findan\\MA_RL_OMM\\macro_agent.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\instancenorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_input_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         return F.instance_norm(\n\u001b[0;32m     58\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\instancenorm.py\u001b[0m in \u001b[0;36m_check_input_dim\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m             )\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             raise ValueError('expected 3D input (got {}D input)'\n\u001b[0m\u001b[0;32m    140\u001b[0m                              .format(input.dim()))\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: expected 3D input (got 1D input)"
     ]
    }
   ],
   "source": [
    "market_data = load_data()\n",
    "train_data = get_train_data(market_data)\n",
    "test_data = get_test_data(market_data)\n",
    "for epoch in tqdm(range(EPOCH_COUNT)):\n",
    "    done = False\n",
    "    cur_epsilon = INIT_EPSILON\n",
    "    environment = Environment(train_data)\n",
    "    macro_agent.sell_assets(0)\n",
    "    while not done:\n",
    "    \n",
    "        now_state = [environment.get_price(), environment.get_indicators(), macro_agent.assets]\n",
    "        macro_agent.q_network.eval()\n",
    "        decision = np.random.rand()\n",
    "        if decision < cur_epsilon:\n",
    "            # epsilon\n",
    "            action = np.random.choice(3)\n",
    "        else:\n",
    "            # 1-epsilon\n",
    "            action = np.argmax(macro_agent.q_network(now_state).detach().numpy())\n",
    "        \n",
    "\n",
    "        # reward algo\n",
    "        cur_reward = reward_algo(macro_agent, action, environment)\n",
    "\n",
    "        environment.taken_action()\n",
    "\n",
    "        next_state = [environment.get_price(), environment.get_indicators(), macro_agent.assets]\n",
    "        done = environment.done\n",
    "        \n",
    "        replay_memory.push(now_state, action, cur_reward, next_state, done)\n",
    "        \n",
    "        BATCH_SIZE = np.min([MINI_BATCH_SIZE, len(replay_memory.memory)])\n",
    "        batch = replay_memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        q = np.empty(BATCH_SIZE)\n",
    "        learning_input = np.empty(BATCH_SIZE, dtype=list)\n",
    "        for i, object in enumerate(batch):\n",
    "            learning_input[i] = [object.state[0], object.state[1], object.state[2]]\n",
    "            if not object.done:\n",
    "                input = [object.next_state[0], object.next_state[1], object.next_state[2]]\n",
    "                act = np.argmax(macro_agent.q_network.forward(input).detach().numpy())\n",
    "                q[i] = object.reward + GAMMA_DISCOUNT * reward_algo(macro_agent, act, environment)\n",
    "            else:\n",
    "                q[i] = object.reward\n",
    "        \n",
    "        macro_agent.q_network.train()\n",
    "        for i, q_i in enumerate(q):\n",
    "            loss = criterion(torch.tensor(q_i, dtype=torch.float32), macro_agent.q_network.forward(learning_input[i]))\n",
    "            loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cur_epsilon = decay_epsilon(cur_epsilon)  \n",
    "    \n",
    "    # test model\n",
    "    macro_agent.q_network.eval()\n",
    "    test_env = Environment(test_data)\n",
    "    macro_agent.sell_assets(price=0)\n",
    "    current_balance = [0]\n",
    "    print('testing')\n",
    "    while not test_env.done:\n",
    "        now_state = [test_env.get_price(), test_env.get_indicators(), macro_agent.assets]\n",
    "        res = macro_agent.q_network(now_state).detach().numpy()\n",
    "        \n",
    "        action = np.argmax(res)\n",
    "        print(res, action)\n",
    "        if action == ACTIONS['BUY']:\n",
    "            macro_agent.buy_asset(test_env.get_price())\n",
    "            current_balance.append(len(macro_agent.assets) * test_env.get_price())\n",
    "        elif action == ACTIONS['SELL']:\n",
    "            earning = macro_agent.sell_assets(test_env.get_price())\n",
    "            current_balance.append(current_balance[-1] + earning)\n",
    "        else:\n",
    "            current_balance.append(len(macro_agent.assets) * test_env.get_price())\n",
    "        test_env.taken_action()\n",
    "\n",
    "\n",
    "\n",
    "    torch.save(macro_agent.q_network.state_dict(), 'q_net_epoch_{}'.format(epoch))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2423, -0.3367, -0.2342,  0.3480, -0.2947,  0.2375,  0.2133],\n",
       "        [ 0.0070,  0.2908,  0.3121,  0.3176,  0.0777,  0.3357,  0.0618],\n",
       "        [ 0.1103,  0.2196, -0.1605, -0.1847, -0.1597,  0.2789,  0.2133]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b2bd16c4a5b7bdb943c513e9c5dbb440fb1e7bca9cf829d440576f5014c581f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
