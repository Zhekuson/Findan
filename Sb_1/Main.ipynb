{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Main.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhekuson/Findan/blob/main/Sb_1/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j06z23TnuylB"
      },
      "source": [
        "# Первоначальная настройка"
      ],
      "id": "j06z23TnuylB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T0SlbaXu3wI"
      },
      "source": [
        "Здесь маунт, импорты и прочее  \n",
        "Не забудь очистить выходы ячеек перед коммитом"
      ],
      "id": "7T0SlbaXu3wI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er-Q4el0uxlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19723484-4082-42ec-dd67-bf077db7e287"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "er-Q4el0uxlC",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7jhh5avw8v-"
      },
      "source": [
        "**Нужно добавить ссылку на мой диск для нашей общей папки по финдану**\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "id": "E7jhh5avw8v-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugHCXDKvvD0R",
        "outputId": "beba57b2-d91c-4562-93de-9c0600f63084"
      },
      "source": [
        "!ls drive/MyDrive/Findan/Sb_1\n",
        "# не очищайте эту ячейку"
      ],
      "id": "ugHCXDKvvD0R",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " data   plots  'Если ты это видишь все ок'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id5v5LhRxU8P"
      },
      "source": [
        "Ссылка на папку Findan должна лежать в корне (в MyDrive)  \n",
        "Если все сделано правильно, то появится доступ к общим файлам в этой папке"
      ],
      "id": "id5v5LhRxU8P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kDjanGQxk0j"
      },
      "source": [
        "Сохраняться должны: **веса моделей/отчеты** - на диске,  \n",
        "**выходы ячеек** - на гитхабе "
      ],
      "id": "4kDjanGQxk0j"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjHG7Fm33tZN"
      },
      "source": [
        "Правила по работе с *общими* ноутбуками  \n",
        "1) **Строго** в отдельных ветках, иначе будет очень плохо  \n",
        "2) PR в main очень аккуратно, мердж конфликты резолвим тщательно     \n",
        "3) Коммиты тоже аккуратные и без лишних выводов ячеек  "
      ],
      "id": "GjHG7Fm33tZN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt1cRT4a53wa"
      },
      "source": [
        "Либо в отдельных ноутбуках, тогда свобода действий"
      ],
      "id": "nt1cRT4a53wa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2HPbspFu-1Q"
      },
      "source": [
        "# Начало"
      ],
      "id": "x2HPbspFu-1Q"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMPKuKAt1T7X"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import sys\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "from datetime import date\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "import keras.backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "from keras.layers import LSTM, GRU, Conv1D, MaxPooling1D\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from operator import add\n",
        "import json"
      ],
      "id": "oMPKuKAt1T7X",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0bPUBFb4pDi"
      },
      "source": [
        "def make_ann_model(window_size):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, activation='relu', input_shape=(window_size, 1)))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "def make_lstm_model(window_size):\n",
        "    model_lstm = Sequential()\n",
        "    model_lstm.add(LSTM(256, input_shape=(window_size, 1), activation='relu', kernel_initializer='lecun_uniform',\n",
        "                        return_sequences=True))\n",
        "    model_lstm.add(Dropout(0.2))\n",
        "    model_lstm.add(LSTM(128, input_shape=(window_size, 1), activation='relu', kernel_initializer='lecun_uniform',\n",
        "                        return_sequences=False))\n",
        "    model_lstm.add(Dropout(0.2))\n",
        "    model_lstm.add(Dense(1, activation='linear'))\n",
        "    model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "    return model_lstm\n",
        "\n",
        "\n",
        "def make_cnn_model(window_size):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(window_size, 1)))\n",
        "    model.add(Conv1D(filters=32, kernel_size=1, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    # model.add(Dense(32,activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def make_gru_model(window_size):\n",
        "    model = Sequential()\n",
        "    model.add(GRU(256, input_shape=(window_size, 1), activation='relu', kernel_initializer='lecun_uniform',\n",
        "                  return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(GRU(128, input_shape=(window_size, 1), activation='relu', kernel_initializer='lecun_uniform',\n",
        "                  return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "    return model\n",
        "\n",
        "def test_model(model, X_train, X_test, y_train, y_test):\n",
        "    y_pred_test_ann = model.predict(X_test)\n",
        "    y_train_pred_ann = model.predict(X_train)\n",
        "    r2_train = mse(y_train, y_train_pred_ann)\n",
        "    r2_test = mse(y_test, y_pred_test_ann)\n",
        "\n",
        "    return r2_train, r2_test"
      ],
      "id": "E0bPUBFb4pDi",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ZWk_AgrRqy04"
      },
      "source": [
        "symbols = ['INFY', 'BHARTIARTL', 'AXISBANK', 'CIPLA', 'HCLTECH', 'HDFC', 'ULTRACEMCO', 'ACC']\n",
        "window_sizes = [3, 5, 7, 9, 11, 13, 15]"
      ],
      "id": "ZWk_AgrRqy04",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "rb9P2aP5qy04"
      },
      "source": [
        "def window_transform(time_series, window_size):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(time_series) - window_size):\n",
        "        X.append(time_series[i:i + window_size])\n",
        "        y.append(time_series[i + window_size])\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ],
      "id": "rb9P2aP5qy04",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "13UhoYWvqy04",
        "outputId": "5fddb262-fccd-49e6-d5d3-9e31dff100f9"
      },
      "source": [
        "for sym in symbols:\n",
        "    json_descriptor = {'stock': sym, 'ann': [], 'gru': [], 'lstm': [], 'cnn': []}\n",
        "    data = pd.read_csv('drive/MyDrive/Findan/Sb_1/data/{}.csv'.format(sym), index_col=0)\n",
        "    print('data loaded for {}!!'.format(sym))\n",
        "    data.index = pd.to_datetime(data.index)\n",
        "    data_frame = data.copy()\n",
        "    df = data_frame[[\"Close\"]]\n",
        "\n",
        "    print(df)\n",
        "    split_date = pd.Timestamp('01-01-2017')\n",
        "\n",
        "    train = df.loc[:split_date]\n",
        "    test = df.loc[split_date:]\n",
        "\n",
        "    sc = MinMaxScaler()\n",
        "    train_sc = sc.fit_transform(train)\n",
        "    test_sc = sc.transform(test)\n",
        "\n",
        "    ann = []\n",
        "    gru = []\n",
        "    lstm = []\n",
        "    cnn = []\n",
        "\n",
        "    for win_sz in window_sizes:\n",
        "        ann_result = []\n",
        "        gru_result = []\n",
        "        lstm_result = []\n",
        "        cnn_result = []\n",
        "        pred_ANN = []\n",
        "        pred_LSTM = []\n",
        "        pred_GRU = []\n",
        "        pred_CNN = []\n",
        "        for i in range(5):\n",
        "            # ''''''''ANN'''''''''''''''''''\n",
        "            X_train, y_train = window_transform(train_sc, win_sz)\n",
        "            X_test, y_test = window_transform(test_sc, win_sz)\n",
        "\n",
        "            model = make_ann_model(win_sz)\n",
        "            early_stop = EarlyStopping(monitor='loss', patience=2, verbose=1)\n",
        "            history = model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=1, callbacks=[early_stop],\n",
        "                                shuffle=False)\n",
        "\n",
        "            train_acc, test_acc = test_model(model, X_train, X_test, y_train, y_test)\n",
        "            y_pred_test_ANN = model.predict(X_test)\n",
        "\n",
        "            ann_result.append(test_acc)\n",
        "            # ''''''''ANN''''''''''''''''''''\n",
        "\n",
        "            # ''''''''CNN''''''''''''''''''''\n",
        "            model = make_cnn_model(win_sz)\n",
        "            early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
        "            history_model_cnn = model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=1, shuffle=False,\n",
        "                                          callbacks=[early_stop])\n",
        "\n",
        "            train_acc, test_acc = test_model(model, X_train, X_test, y_train, y_test)\n",
        "            y_pred_test_CNN = model.predict(X_test)\n",
        "            cnn_result.append(test_acc)\n",
        "            # ''''''''CNN''''''''''''''''''''\n",
        "\n",
        "            X_tr_t = X_train.reshape(X_train.shape[0], win_sz, 1)\n",
        "            X_tst_t = X_test.reshape(X_test.shape[0], win_sz, 1)\n",
        "\n",
        "            # ''''''''''LSTM''''''''''''''''''\n",
        "            model = make_lstm_model(win_sz)\n",
        "            early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
        "            history_model_lstm = model.fit(X_tr_t, y_train, epochs=200, batch_size=32, verbose=1, shuffle=False,\n",
        "                                           callbacks=[early_stop])\n",
        "\n",
        "            train_acc, test_acc = test_model(model, X_tr_t, X_tst_t, y_train, y_test)\n",
        "            y_pred_test_LSTM = model.predict(X_tst_t)\n",
        "            lstm_result.append(test_acc)\n",
        "            # \"''''''''LSTM'''''''''''''''''''\n",
        "\n",
        "            # ''''''''GRU'''''''''''''''''''''\n",
        "            model = make_gru_model(win_sz)\n",
        "            early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
        "            history_model_gru = model.fit(X_tr_t, y_train, epochs=200, batch_size=32, verbose=1, shuffle=False,\n",
        "                                          callbacks=[early_stop])\n",
        "\n",
        "            train_acc, test_acc = test_model(model, X_tr_t, X_tst_t, y_train, y_test)\n",
        "            y_pred_test_GRU = model.predict(X_tst_t)\n",
        "            gru_result.append(test_acc)\n",
        "            # ''''''''GRU'''''''''''''''''''''\n",
        "\n",
        "            pred_ANN.append(y_pred_test_ANN)\n",
        "            pred_CNN.append(y_pred_test_CNN)\n",
        "            pred_LSTM.append(y_pred_test_LSTM)\n",
        "            pred_GRU.append(y_pred_test_GRU)\n",
        "        # update optimal window size param\n",
        "        ann.append([win_sz, min(ann_result), np.mean(ann_result), np.std(ann_result)])\n",
        "        gru.append([win_sz, min(gru_result), np.mean(gru_result), np.std(gru_result)])\n",
        "        lstm.append([win_sz, min(lstm_result), np.mean(lstm_result), np.std(lstm_result)])\n",
        "        cnn.append([win_sz, min(cnn_result), np.mean(cnn_result), np.std(cnn_result)])\n",
        "\n",
        "        plot_ann = [0] * len(pred_ANN[0])\n",
        "        for pred in pred_ANN:\n",
        "            plot_ann = list(map(add, plot_ann, pred))\n",
        "\n",
        "        for i in range(len(plot_ann)):\n",
        "            plot_ann[i] = plot_ann[i] / 5\n",
        "\n",
        "        plot_lstm = [0] * len(pred_LSTM[0])\n",
        "        for pred in pred_LSTM:\n",
        "            plot_lstm = list(map(add, plot_lstm, pred))\n",
        "\n",
        "        for i in range(len(plot_lstm)):\n",
        "            plot_lstm[i] = plot_lstm[i] / 5\n",
        "\n",
        "        plot_gru = [0] * len(pred_GRU[0])\n",
        "        for pred in pred_GRU:\n",
        "            plot_gru = list(map(add, plot_gru, pred))\n",
        "\n",
        "        for i in range(len(plot_gru)):\n",
        "            plot_gru[i] = plot_gru[i] / 5\n",
        "\n",
        "        plot_cnn = [0] * len(pred_CNN[0])\n",
        "        for pred in pred_CNN:\n",
        "            plot_cnn = list(map(add, plot_cnn, pred))\n",
        "\n",
        "        for i in range(len(plot_cnn)):\n",
        "            plot_cnn[i] = plot_cnn[i] / 5\n",
        "\n",
        "        # save prediction plots\n",
        "        plt.plot(y_test, '-', label='True Values', color='#1b9e77')\n",
        "        plt.plot(plot_ann, label='ANN Prediction', color='#d95f02')\n",
        "        plt.plot(plot_cnn, ':', label='CNN Prediction', color='#7570b3')\n",
        "        plt.plot(plot_lstm, label='LSTM Prediction', color='#e7298a')\n",
        "        plt.plot(plot_gru, label='GRU Prediction', color='#66a61e')\n",
        "        plt.title(\"Prediction\")\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel('Normalized Stock Prices')\n",
        "        plt.legend()\n",
        "        plt.savefig('drive/MyDrive/Findan/Sb_1/plots/single_step/' + sym + ' ' + 'window_sz ' + str(win_sz))\n",
        "        plt.clf()"
      ],
      "id": "13UhoYWvqy04",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mВыходные данные были обрезаны до нескольких последних строк (5000).\u001b[0m\n",
            "Epoch 3/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0020\n",
            "Epoch 4/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 5/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 6/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 7/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0015\n",
            "Epoch 8/200\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 0.0014\n",
            "Epoch 10/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 12/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 13/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 14/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 9.9120e-04\n",
            "Epoch 15/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 9.8484e-04\n",
            "Epoch 16/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 9.1712e-04\n",
            "Epoch 17/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 9.4476e-04\n",
            "Epoch 18/200\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 8.2608e-04\n",
            "Epoch 19/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 9.1566e-04\n",
            "Epoch 20/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 7.2166e-04\n",
            "Epoch 21/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 9.0304e-04\n",
            "Epoch 22/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 6.4038e-04\n",
            "Epoch 23/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 8.7219e-04\n",
            "Epoch 24/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 6.4636e-04\n",
            "Epoch 25/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 7.1193e-04\n",
            "Epoch 26/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 6.8469e-04\n",
            "Epoch 27/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 6.2833e-04\n",
            "Epoch 28/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 7.0552e-04\n",
            "Epoch 29/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 6.6107e-04\n",
            "Epoch 30/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 6.6766e-04\n",
            "Epoch 31/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 7.0415e-04\n",
            "Epoch 32/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 6.5020e-04\n",
            "Epoch 00032: early stopping\n",
            "Epoch 1/200\n",
            "117/117 [==============================] - 16s 113ms/step - loss: 0.0222\n",
            "Epoch 2/200\n",
            "117/117 [==============================] - 13s 112ms/step - loss: 0.0084\n",
            "Epoch 3/200\n",
            "117/117 [==============================] - 13s 115ms/step - loss: 0.0047\n",
            "Epoch 4/200\n",
            "117/117 [==============================] - 13s 115ms/step - loss: 0.0032\n",
            "Epoch 5/200\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 0.0024\n",
            "Epoch 6/200\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 0.0026\n",
            "Epoch 7/200\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 0.0028\n",
            "Epoch 8/200\n",
            "117/117 [==============================] - 14s 116ms/step - loss: 0.0020\n",
            "Epoch 9/200\n",
            "117/117 [==============================] - 13s 112ms/step - loss: 0.0020\n",
            "Epoch 10/200\n",
            "117/117 [==============================] - 13s 114ms/step - loss: 0.0025\n",
            "Epoch 11/200\n",
            "117/117 [==============================] - 13s 112ms/step - loss: 0.0018\n",
            "Epoch 12/200\n",
            "117/117 [==============================] - 13s 114ms/step - loss: 0.0015\n",
            "Epoch 13/200\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 0.0021\n",
            "Epoch 14/200\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 0.0016\n",
            "Epoch 15/200\n",
            "117/117 [==============================] - 13s 114ms/step - loss: 0.0019\n",
            "Epoch 16/200\n",
            "117/117 [==============================] - 13s 112ms/step - loss: 0.0015\n",
            "Epoch 17/200\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 0.0018\n",
            "Epoch 18/200\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 0.0015\n",
            "Epoch 19/200\n",
            "117/117 [==============================] - 13s 112ms/step - loss: 0.0017\n",
            "Epoch 20/200\n",
            "117/117 [==============================] - 13s 114ms/step - loss: 0.0014\n",
            "Epoch 21/200\n",
            "117/117 [==============================] - 14s 116ms/step - loss: 0.0014\n",
            "Epoch 22/200\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 0.0016\n",
            "Epoch 23/200\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 0.0015\n",
            "Epoch 24/200\n",
            "117/117 [==============================] - 13s 114ms/step - loss: 0.0013\n",
            "Epoch 25/200\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 0.0014\n",
            "Epoch 26/200\n",
            "117/117 [==============================] - 13s 114ms/step - loss: 0.0020\n",
            "Epoch 27/200\n",
            "117/117 [==============================] - 13s 111ms/step - loss: 0.0014\n",
            "Epoch 28/200\n",
            "117/117 [==============================] - 13s 112ms/step - loss: 0.0014\n",
            "Epoch 29/200\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 0.0017\n",
            "Epoch 00029: early stopping\n",
            "Epoch 1/200\n",
            "117/117 [==============================] - 14s 97ms/step - loss: 0.0253\n",
            "Epoch 2/200\n",
            "117/117 [==============================] - 11s 95ms/step - loss: 0.0047\n",
            "Epoch 3/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0032\n",
            "Epoch 4/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0036\n",
            "Epoch 5/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0031\n",
            "Epoch 6/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0035\n",
            "Epoch 7/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0030\n",
            "Epoch 8/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0018\n",
            "Epoch 9/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0019\n",
            "Epoch 10/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0018\n",
            "Epoch 11/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0022\n",
            "Epoch 12/200\n",
            "117/117 [==============================] - 11s 95ms/step - loss: 0.0024\n",
            "Epoch 13/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0023\n",
            "Epoch 14/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0020\n",
            "Epoch 15/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0016\n",
            "Epoch 16/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0014\n",
            "Epoch 17/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0014\n",
            "Epoch 18/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0016\n",
            "Epoch 19/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0018\n",
            "Epoch 20/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0018\n",
            "Epoch 21/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0018\n",
            "Epoch 22/200\n",
            "117/117 [==============================] - 11s 95ms/step - loss: 0.0020\n",
            "Epoch 00022: early stopping\n",
            "Epoch 1/200\n",
            "117/117 [==============================] - 1s 3ms/step - loss: 0.0094\n",
            "Epoch 2/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 3/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 4/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 5/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 6/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 7/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 8/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0020\n",
            "Epoch 9/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 10/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 11/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 12/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 13/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 14/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 15/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 9.9853e-04\n",
            "Epoch 16/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0010\n",
            "Epoch 17/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 9.0246e-04\n",
            "Epoch 18/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 8.8875e-04\n",
            "Epoch 19/200\n",
            "117/117 [==============================] - 0s 2ms/step - loss: 8.5099e-04\n",
            "Epoch 20/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 8.7279e-04\n",
            "Epoch 21/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 8.0769e-04\n",
            "Epoch 22/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 9.6431e-04\n",
            "Epoch 23/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 7.8353e-04\n",
            "Epoch 24/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 25/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 9.3134e-04\n",
            "Epoch 00025: early stopping\n",
            "Epoch 1/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0186\n",
            "Epoch 2/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0033\n",
            "Epoch 3/200\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 0.0024\n",
            "Epoch 4/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0023\n",
            "Epoch 5/200\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 0.0022\n",
            "Epoch 6/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0022\n",
            "Epoch 7/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0019\n",
            "Epoch 8/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0017\n",
            "Epoch 9/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0015\n",
            "Epoch 10/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 11/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0011\n",
            "Epoch 12/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 9.3274e-04\n",
            "Epoch 14/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0013\n",
            "Epoch 15/200\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 8.1914e-04\n",
            "Epoch 16/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 17/200\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 8.6656e-04\n",
            "Epoch 18/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 19/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 7.6616e-04\n",
            "Epoch 20/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0011\n",
            "Epoch 21/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 6.6119e-04\n",
            "Epoch 22/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 8.7707e-04\n",
            "Epoch 23/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 5.7738e-04\n",
            "Epoch 24/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 9.3941e-04\n",
            "Epoch 25/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 6.0574e-04\n",
            "Epoch 26/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 6.8261e-04\n",
            "Epoch 27/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 7.0267e-04\n",
            "Epoch 28/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 7.6621e-04\n",
            "Epoch 00028: early stopping\n",
            "Epoch 1/200\n",
            "117/117 [==============================] - 17s 119ms/step - loss: 0.0309\n",
            "Epoch 2/200\n",
            "117/117 [==============================] - 14s 120ms/step - loss: 0.0104\n",
            "Epoch 3/200\n",
            "117/117 [==============================] - 14s 119ms/step - loss: 0.0045\n",
            "Epoch 4/200\n",
            "117/117 [==============================] - 14s 121ms/step - loss: 0.0037\n",
            "Epoch 5/200\n",
            "117/117 [==============================] - 14s 118ms/step - loss: 0.0027\n",
            "Epoch 6/200\n",
            "117/117 [==============================] - 14s 119ms/step - loss: 0.0024\n",
            "Epoch 7/200\n",
            "117/117 [==============================] - 14s 121ms/step - loss: 0.0019\n",
            "Epoch 8/200\n",
            "117/117 [==============================] - 14s 118ms/step - loss: 0.0023\n",
            "Epoch 9/200\n",
            "117/117 [==============================] - 14s 116ms/step - loss: 0.0020\n",
            "Epoch 10/200\n",
            "117/117 [==============================] - 14s 117ms/step - loss: 0.0036\n",
            "Epoch 11/200\n",
            "117/117 [==============================] - 13s 115ms/step - loss: 0.0018\n",
            "Epoch 12/200\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 0.0023\n",
            "Epoch 13/200\n",
            "117/117 [==============================] - 14s 116ms/step - loss: 0.0018\n",
            "Epoch 14/200\n",
            "117/117 [==============================] - 14s 115ms/step - loss: 0.0016\n",
            "Epoch 15/200\n",
            "117/117 [==============================] - 13s 115ms/step - loss: 0.0021\n",
            "Epoch 16/200\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 0.0014\n",
            "Epoch 17/200\n",
            "117/117 [==============================] - 13s 115ms/step - loss: 0.0017\n",
            "Epoch 18/200\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 0.0017\n",
            "Epoch 19/200\n",
            "117/117 [==============================] - 14s 116ms/step - loss: 0.0015\n",
            "Epoch 20/200\n",
            "117/117 [==============================] - 14s 117ms/step - loss: 0.0015\n",
            "Epoch 21/200\n",
            "117/117 [==============================] - 14s 115ms/step - loss: 0.0017\n",
            "Epoch 00021: early stopping\n",
            "Epoch 1/200\n",
            "117/117 [==============================] - 14s 95ms/step - loss: 0.0271\n",
            "Epoch 2/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0034\n",
            "Epoch 3/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0054\n",
            "Epoch 4/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0058\n",
            "Epoch 5/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0023\n",
            "Epoch 6/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0020\n",
            "Epoch 7/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0024\n",
            "Epoch 8/200\n",
            "117/117 [==============================] - 11s 92ms/step - loss: 0.0026\n",
            "Epoch 9/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0018\n",
            "Epoch 10/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0018\n",
            "Epoch 11/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0024\n",
            "Epoch 12/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0025\n",
            "Epoch 13/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0022\n",
            "Epoch 14/200\n",
            "117/117 [==============================] - 11s 92ms/step - loss: 0.0017\n",
            "Epoch 15/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0016\n",
            "Epoch 16/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0016\n",
            "Epoch 17/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0016\n",
            "Epoch 18/200\n",
            "117/117 [==============================] - 11s 93ms/step - loss: 0.0023\n",
            "Epoch 19/200\n",
            "117/117 [==============================] - 11s 94ms/step - loss: 0.0024\n",
            "Epoch 20/200\n",
            "117/117 [==============================] - 11s 95ms/step - loss: 0.0014\n",
            "Epoch 21/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0013\n",
            "Epoch 22/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0013\n",
            "Epoch 23/200\n",
            "117/117 [==============================] - 11s 98ms/step - loss: 0.0016\n",
            "Epoch 24/200\n",
            "117/117 [==============================] - 11s 95ms/step - loss: 0.0014\n",
            "Epoch 25/200\n",
            "117/117 [==============================] - 11s 95ms/step - loss: 0.0015\n",
            "Epoch 26/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0015\n",
            "Epoch 00026: early stopping\n",
            "Epoch 1/200\n",
            "117/117 [==============================] - 1s 3ms/step - loss: 0.0445\n",
            "Epoch 2/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 3/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 4/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 5/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 6/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 8/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 9/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 10/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0010\n",
            "Epoch 11/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 9.7873e-04\n",
            "Epoch 12/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 9.3719e-04\n",
            "Epoch 13/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 9.4136e-04\n",
            "Epoch 14/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 8.6792e-04\n",
            "Epoch 15/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0010\n",
            "Epoch 16/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 8.7419e-04\n",
            "Epoch 00016: early stopping\n",
            "Epoch 1/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0413\n",
            "Epoch 2/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0031\n",
            "Epoch 3/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 4/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0021\n",
            "Epoch 5/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0019\n",
            "Epoch 6/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 7/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 8/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0017\n",
            "Epoch 9/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 10/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 11/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 9.6068e-04\n",
            "Epoch 12/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0011\n",
            "Epoch 13/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 8.9908e-04\n",
            "Epoch 14/200\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 0.0014\n",
            "Epoch 15/200\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 0.0010\n",
            "Epoch 16/200\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 0.0011\n",
            "Epoch 17/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 7.9980e-04\n",
            "Epoch 18/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 19/200\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 9.9447e-04\n",
            "Epoch 20/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 21/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 9.9351e-04\n",
            "Epoch 22/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 00022: early stopping\n",
            "Epoch 1/200\n",
            "117/117 [==============================] - 16s 119ms/step - loss: 0.0276\n",
            "Epoch 2/200\n",
            "117/117 [==============================] - 14s 118ms/step - loss: 0.0099\n",
            "Epoch 3/200\n",
            "117/117 [==============================] - 14s 116ms/step - loss: 0.0055\n",
            "Epoch 4/200\n",
            "117/117 [==============================] - 14s 118ms/step - loss: 0.0034\n",
            "Epoch 5/200\n",
            "117/117 [==============================] - 14s 117ms/step - loss: 0.0034\n",
            "Epoch 6/200\n",
            "117/117 [==============================] - 14s 117ms/step - loss: 0.0027\n",
            "Epoch 7/200\n",
            "117/117 [==============================] - 14s 117ms/step - loss: 0.0028\n",
            "Epoch 8/200\n",
            "117/117 [==============================] - 14s 118ms/step - loss: 0.0021\n",
            "Epoch 9/200\n",
            "117/117 [==============================] - 14s 116ms/step - loss: 0.0027\n",
            "Epoch 10/200\n",
            "117/117 [==============================] - 14s 117ms/step - loss: 0.0026\n",
            "Epoch 11/200\n",
            "117/117 [==============================] - 14s 116ms/step - loss: 0.0022\n",
            "Epoch 12/200\n",
            "117/117 [==============================] - 14s 116ms/step - loss: 0.0021\n",
            "Epoch 13/200\n",
            "117/117 [==============================] - 14s 117ms/step - loss: 0.0025\n",
            "Epoch 14/200\n",
            "117/117 [==============================] - 14s 116ms/step - loss: 0.0017\n",
            "Epoch 15/200\n",
            "117/117 [==============================] - 14s 116ms/step - loss: 0.0017\n",
            "Epoch 16/200\n",
            "117/117 [==============================] - 14s 115ms/step - loss: 0.0020\n",
            "Epoch 17/200\n",
            "117/117 [==============================] - 14s 116ms/step - loss: 0.0015\n",
            "Epoch 18/200\n",
            "117/117 [==============================] - 13s 114ms/step - loss: 0.0016\n",
            "Epoch 19/200\n",
            "117/117 [==============================] - 14s 117ms/step - loss: 0.0018\n",
            "Epoch 20/200\n",
            "117/117 [==============================] - 14s 118ms/step - loss: 0.0015\n",
            "Epoch 21/200\n",
            "117/117 [==============================] - 14s 115ms/step - loss: 0.0019\n",
            "Epoch 22/200\n",
            "117/117 [==============================] - 14s 115ms/step - loss: 0.0015\n",
            "Epoch 23/200\n",
            "117/117 [==============================] - 13s 115ms/step - loss: 0.0018\n",
            "Epoch 24/200\n",
            "117/117 [==============================] - 14s 116ms/step - loss: 0.0013\n",
            "Epoch 25/200\n",
            "117/117 [==============================] - 13s 115ms/step - loss: 0.0012\n",
            "Epoch 26/200\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 0.0016\n",
            "Epoch 27/200\n",
            "117/117 [==============================] - 14s 116ms/step - loss: 0.0013\n",
            "Epoch 28/200\n",
            "117/117 [==============================] - 13s 115ms/step - loss: 0.0013\n",
            "Epoch 29/200\n",
            "117/117 [==============================] - 13s 112ms/step - loss: 0.0012\n",
            "Epoch 30/200\n",
            "117/117 [==============================] - 13s 114ms/step - loss: 0.0014\n",
            "Epoch 31/200\n",
            "117/117 [==============================] - 13s 114ms/step - loss: 0.0013\n",
            "Epoch 32/200\n",
            "117/117 [==============================] - 14s 116ms/step - loss: 0.0013\n",
            "Epoch 33/200\n",
            "117/117 [==============================] - 14s 115ms/step - loss: 0.0019\n",
            "Epoch 34/200\n",
            "117/117 [==============================] - 13s 114ms/step - loss: 0.0022\n",
            "Epoch 00034: early stopping\n",
            "Epoch 1/200\n",
            "117/117 [==============================] - 14s 96ms/step - loss: 0.0280\n",
            "Epoch 2/200\n",
            "117/117 [==============================] - 11s 95ms/step - loss: 0.0038\n",
            "Epoch 3/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0049\n",
            "Epoch 4/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0037\n",
            "Epoch 5/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0043\n",
            "Epoch 6/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0022\n",
            "Epoch 7/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0021\n",
            "Epoch 8/200\n",
            "117/117 [==============================] - 11s 97ms/step - loss: 0.0026\n",
            "Epoch 9/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0023\n",
            "Epoch 10/200\n",
            "117/117 [==============================] - 11s 97ms/step - loss: 0.0028\n",
            "Epoch 11/200\n",
            "117/117 [==============================] - 11s 98ms/step - loss: 0.0026\n",
            "Epoch 12/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0018\n",
            "Epoch 13/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0020\n",
            "Epoch 14/200\n",
            "117/117 [==============================] - 11s 97ms/step - loss: 0.0021\n",
            "Epoch 15/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0026\n",
            "Epoch 16/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0019\n",
            "Epoch 17/200\n",
            "117/117 [==============================] - 11s 97ms/step - loss: 0.0016\n",
            "Epoch 18/200\n",
            "117/117 [==============================] - 11s 97ms/step - loss: 0.0020\n",
            "Epoch 19/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0016\n",
            "Epoch 20/200\n",
            "117/117 [==============================] - 11s 97ms/step - loss: 0.0016\n",
            "Epoch 21/200\n",
            "117/117 [==============================] - 11s 97ms/step - loss: 0.0014\n",
            "Epoch 22/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0018\n",
            "Epoch 23/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0018\n",
            "Epoch 24/200\n",
            "117/117 [==============================] - 11s 97ms/step - loss: 0.0014\n",
            "Epoch 25/200\n",
            "117/117 [==============================] - 11s 96ms/step - loss: 0.0014\n",
            "Epoch 26/200\n",
            "117/117 [==============================] - 11s 95ms/step - loss: 0.0016\n",
            "Epoch 00026: early stopping\n",
            "data loaded for BHARTIARTL!!\n",
            "             Close\n",
            "Date              \n",
            "2002-02-18   44.35\n",
            "2002-02-19   41.70\n",
            "2002-02-20   41.25\n",
            "2002-02-21   42.40\n",
            "2002-02-22   43.30\n",
            "...            ...\n",
            "2019-01-09  334.50\n",
            "2019-01-10  337.30\n",
            "2019-01-11  335.10\n",
            "2019-01-14  331.60\n",
            "2019-01-15  337.75\n",
            "\n",
            "[4211 rows x 1 columns]\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0307\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0160\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0091\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0044\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.1523e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.5040e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 3.7066e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.4001e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.2829e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.2648e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.3105e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.4179e-04\n",
            "Epoch 00013: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0011\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.0818e-04\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.7913e-04\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0515e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.2186e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.8836e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.8475e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.9695e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.1599e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.3285e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.5932e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.8240e-04\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 7s 37ms/step - loss: 0.0020\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 4s 36ms/step - loss: 0.0013\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 0.0013\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 4s 36ms/step - loss: 0.0010\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 0.0012\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 4s 38ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 4s 36ms/step - loss: 0.0011\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 0.0010\n",
            "Epoch 00010: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 7s 32ms/step - loss: 0.0029\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0021\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0017\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0017\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0015\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0014\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0012\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0012\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0014\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0012\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0010\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0011\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0014\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0013\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0011\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0011\n",
            "Epoch 00018: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0301\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0145\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0076\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.7820e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.3373e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.3855e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.9668e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.7856e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.7350e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 2.7488e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.8283e-04\n",
            "Epoch 00013: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0021\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0038\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 00013: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 7s 36ms/step - loss: 0.0020\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 0.0020\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 4s 36ms/step - loss: 0.0016\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 4s 36ms/step - loss: 0.0016\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 4s 36ms/step - loss: 0.0018\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 4s 36ms/step - loss: 0.0019\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 4s 36ms/step - loss: 0.0015\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 0.0015\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 4s 35ms/step - loss: 0.0015\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 4s 36ms/step - loss: 0.0013\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 4s 35ms/step - loss: 0.0012\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 4s 36ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 0.0010\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 9.6743e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 4s 38ms/step - loss: 0.0011\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 4s 36ms/step - loss: 8.8301e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 4s 38ms/step - loss: 8.6140e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 4s 38ms/step - loss: 8.8940e-04\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 9.0071e-04\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 4s 38ms/step - loss: 8.9327e-04\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 8.6665e-04\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 5s 39ms/step - loss: 0.0010\n",
            "Epoch 00022: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 7s 32ms/step - loss: 0.0025\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0016\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0018\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 0.0014\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0013\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 0.0015\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0013\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 0.0013\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 0.0015\n",
            "Epoch 00010: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0118\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0080\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.8491e-04\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.3621e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.4790e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.1842e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.0159e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.9564e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.9849e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.0795e-04\n",
            "Epoch 00012: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0013\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.4580e-04\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.1881e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.3614e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.3487e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.3045e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.2286e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.0910e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.6911e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.1186e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.2824e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.2712e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.2077e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 4.1615e-04\n",
            "Epoch 00017: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 8s 40ms/step - loss: 0.0021\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 5s 39ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 5s 40ms/step - loss: 0.0018\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 5s 39ms/step - loss: 0.0014\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 5s 41ms/step - loss: 0.0013\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 5s 41ms/step - loss: 0.0013\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 5s 41ms/step - loss: 0.0014\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 5s 42ms/step - loss: 0.0013\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 5s 41ms/step - loss: 0.0014\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 4s 39ms/step - loss: 0.0015\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 4s 38ms/step - loss: 0.0013\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 5s 41ms/step - loss: 0.0014\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 5s 40ms/step - loss: 0.0012\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 5s 39ms/step - loss: 0.0011\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 8.9963e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 5s 39ms/step - loss: 0.0010\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 5s 40ms/step - loss: 8.7470e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 5s 39ms/step - loss: 9.6374e-04\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 4s 38ms/step - loss: 9.9085e-04\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 5s 39ms/step - loss: 8.8248e-04\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 9.7108e-04\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 9.1942e-04\n",
            "Epoch 00022: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 7s 33ms/step - loss: 0.0030\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 0.0024\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 0.0016\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 0.0012\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0013\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 0.0012\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 0.0010\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 0.0011\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0011\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0011\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 8.6028e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0011\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0011\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0010\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0011\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0013\n",
            "Epoch 00018: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0012\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0011\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.2719e-04\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.2050e-04\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.2164e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.7408e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.4897e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.4621e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.5620e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.7098e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.8691e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.0049e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0979e-04\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 7s 38ms/step - loss: 0.0019\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 5s 39ms/step - loss: 0.0017\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 4s 38ms/step - loss: 0.0020\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 5s 39ms/step - loss: 0.0017\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 4s 38ms/step - loss: 0.0020\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 0.0020\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 4s 38ms/step - loss: 0.0017\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 5s 39ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 5s 41ms/step - loss: 0.0017\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 4s 38ms/step - loss: 0.0014\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 5s 39ms/step - loss: 0.0014\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 4s 38ms/step - loss: 0.0016\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 0.0018\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 4s 36ms/step - loss: 0.0017\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 0.0019\n",
            "Epoch 00015: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 7s 32ms/step - loss: 0.0026\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0021\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0017\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0013\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0013\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0013\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0012\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0011\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0010\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0011\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0010\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 9.1551e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0011\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 9.4598e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 9.9992e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 8.7764e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 9.0414e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 0.0012\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0010\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0012\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0010\n",
            "Epoch 00021: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0022\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0039\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0011\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.2407e-04\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.4186e-04\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.9984e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.1443e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.7205e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.5481e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.5614e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.6907e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.8812e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0951e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.2990e-04\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 7s 38ms/step - loss: 0.0020\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 4s 38ms/step - loss: 0.0016\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 5s 39ms/step - loss: 0.0017\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 4s 38ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 0.0017\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 0.0016\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 4s 37ms/step - loss: 0.0016\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 4s 36ms/step - loss: 0.0017\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 4s 36ms/step - loss: 0.0016\n",
            "Epoch 00009: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 7s 31ms/step - loss: 0.0024\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0017\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 4s 30ms/step - loss: 0.0014\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0013\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0012\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0013\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0011\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 9.9272e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0010\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 0.0011\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 9.6492e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 9.8589e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 9.6274e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 9.1834e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 9.5520e-04\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 4s 31ms/step - loss: 8.6384e-04\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 0.0010\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 4s 30ms/step - loss: 0.0012\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 9.3644e-04\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 4s 33ms/step - loss: 8.8714e-04\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 4s 32ms/step - loss: 9.8915e-04\n",
            "Epoch 00024: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0431\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0082\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0052\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.1908e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.6586e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.3831e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.5095e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.9568e-04\n",
            "Epoch 00010: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.3580e-04\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.8211e-04\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.5051e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0601e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0002e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.2004e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.2034e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.1179e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.9996e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.4178e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 4.9082e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.2155e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.3109e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.2438e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0767e-04\n",
            "Epoch 00018: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 10s 62ms/step - loss: 0.0023\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0016\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0024\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0020\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0021\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0019\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 7s 60ms/step - loss: 0.0018\n",
            "Epoch 00007: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 8s 47ms/step - loss: 0.0027\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 5s 46ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 6s 49ms/step - loss: 0.0013\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 5s 47ms/step - loss: 0.0016\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 5s 44ms/step - loss: 0.0013\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 6s 50ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 5s 47ms/step - loss: 0.0011\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 5s 47ms/step - loss: 0.0012\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 5s 47ms/step - loss: 0.0012\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 5s 47ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 6s 48ms/step - loss: 9.7985e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 6s 52ms/step - loss: 9.8973e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 6s 49ms/step - loss: 9.2094e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 5s 47ms/step - loss: 0.0010\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 5s 46ms/step - loss: 9.7567e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 6s 50ms/step - loss: 0.0010\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 6s 55ms/step - loss: 8.6441e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 6s 51ms/step - loss: 0.0010\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 6s 49ms/step - loss: 0.0010\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 5s 46ms/step - loss: 0.0012\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 5s 47ms/step - loss: 0.0013\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 6s 48ms/step - loss: 0.0013\n",
            "Epoch 00022: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0050\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0013\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.5388e-04\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.6699e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.5834e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0283e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0344e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.2303e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.4161e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.7577e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.1737e-04\n",
            "Epoch 00013: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 10s 60ms/step - loss: 0.0030\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0016\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0017\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 7s 60ms/step - loss: 0.0016\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0015\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0017\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0019\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0015\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 7s 60ms/step - loss: 0.0016\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0019\n",
            "Epoch 00010: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 8s 45ms/step - loss: 0.0026\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 6s 50ms/step - loss: 0.0020\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 6s 52ms/step - loss: 0.0016\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 5s 47ms/step - loss: 0.0013\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 6s 48ms/step - loss: 0.0016\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 6s 48ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 6s 52ms/step - loss: 0.0012\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 6s 50ms/step - loss: 0.0013\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 6s 51ms/step - loss: 0.0011\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 6s 53ms/step - loss: 0.0011\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 5s 46ms/step - loss: 0.0011\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 6s 51ms/step - loss: 0.0011\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 6s 50ms/step - loss: 0.0013\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 6s 52ms/step - loss: 0.0014\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 6s 54ms/step - loss: 0.0014\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 6s 51ms/step - loss: 0.0013\n",
            "Epoch 00016: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0021\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0038\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0020\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0020\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0030\n",
            "Epoch 00011: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 10s 62ms/step - loss: 0.0031\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0022\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0021\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 0.0019\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0016\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0015\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0014\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0014\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0014\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0015\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 0.0014\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0010\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0012\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0011\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0010\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 0.0010\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0011\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 7s 60ms/step - loss: 0.0011\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0010\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 8.1192e-04\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 9.1457e-04\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 8.1969e-04\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 7.7533e-04\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 9.8531e-04\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 9.3937e-04\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 9.7038e-04\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 7s 60ms/step - loss: 8.6052e-04\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 7s 60ms/step - loss: 7.6563e-04\n",
            "Epoch 31/200\n",
            "116/116 [==============================] - 7s 60ms/step - loss: 7.3623e-04\n",
            "Epoch 32/200\n",
            "116/116 [==============================] - 7s 58ms/step - loss: 8.2396e-04\n",
            "Epoch 33/200\n",
            "116/116 [==============================] - 7s 59ms/step - loss: 7.2614e-04\n",
            "Epoch 34/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 7.0601e-04\n",
            "Epoch 35/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 8.5227e-04\n",
            "Epoch 36/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 7.7585e-04\n",
            "Epoch 37/200\n",
            "116/116 [==============================] - 7s 60ms/step - loss: 8.0863e-04\n",
            "Epoch 38/200\n",
            "116/116 [==============================] - 7s 59ms/step - loss: 7.3895e-04\n",
            "Epoch 39/200\n",
            "116/116 [==============================] - 7s 60ms/step - loss: 6.5063e-04\n",
            "Epoch 40/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 6.9083e-04\n",
            "Epoch 41/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 6.8136e-04\n",
            "Epoch 42/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 6.7568e-04\n",
            "Epoch 43/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 7.0763e-04\n",
            "Epoch 44/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 7.0936e-04\n",
            "Epoch 00044: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 9s 49ms/step - loss: 0.0030\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 6s 53ms/step - loss: 0.0020\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 6s 52ms/step - loss: 0.0018\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 6s 50ms/step - loss: 0.0016\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 6s 50ms/step - loss: 0.0018\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 6s 49ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 6s 50ms/step - loss: 0.0013\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 6s 50ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 5s 45ms/step - loss: 0.0014\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 6s 51ms/step - loss: 0.0014\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 6s 48ms/step - loss: 0.0012\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 6s 51ms/step - loss: 0.0014\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 6s 48ms/step - loss: 0.0013\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 6s 51ms/step - loss: 0.0012\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 6s 50ms/step - loss: 0.0012\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 6s 50ms/step - loss: 0.0014\n",
            "Epoch 00016: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0337\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0139\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0081\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.2325e-04\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.0083e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.6857e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.9961e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.7156e-04\n",
            "Epoch 00009: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0012\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.6614e-04\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.8765e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.7993e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.2835e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0540e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0134e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0738e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.2179e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.2866e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.3136e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.2935e-04\n",
            "Epoch 00015: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 10s 60ms/step - loss: 0.0024\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 7s 60ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 7s 59ms/step - loss: 0.0019\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 7s 60ms/step - loss: 0.0013\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 7s 60ms/step - loss: 0.0014\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 7s 59ms/step - loss: 0.0016\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 7s 59ms/step - loss: 0.0018\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 7s 59ms/step - loss: 0.0016\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 7s 59ms/step - loss: 0.0017\n",
            "Epoch 00009: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 8s 46ms/step - loss: 0.0022\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 5s 45ms/step - loss: 0.0016\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 5s 42ms/step - loss: 0.0019\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 5s 44ms/step - loss: 0.0013\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 5s 46ms/step - loss: 0.0013\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 5s 43ms/step - loss: 0.0012\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 5s 46ms/step - loss: 0.0013\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 5s 45ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 5s 45ms/step - loss: 0.0013\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 5s 45ms/step - loss: 0.0014\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 5s 47ms/step - loss: 0.0014\n",
            "Epoch 00011: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0121\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0115\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0055\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.3226e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 4.7264e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.0692e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.7800e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.6710e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.6521e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.6974e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.7908e-04\n",
            "Epoch 00013: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0013\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.3979e-04\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.0875e-04\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.2579e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.2645e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.4815e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.4740e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.6172e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.7607e-04\n",
            "Epoch 00011: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 10s 58ms/step - loss: 0.0029\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 7s 59ms/step - loss: 0.0020\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 7s 58ms/step - loss: 0.0018\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 7s 60ms/step - loss: 0.0022\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 7s 59ms/step - loss: 0.0022\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 7s 59ms/step - loss: 0.0019\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 7s 58ms/step - loss: 0.0018\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 7s 59ms/step - loss: 0.0018\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 7s 59ms/step - loss: 0.0020\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 7s 59ms/step - loss: 0.0021\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0022\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 7s 59ms/step - loss: 0.0026\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 7s 60ms/step - loss: 0.0033\n",
            "Epoch 00013: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 8s 46ms/step - loss: 0.0029\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 5s 46ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 5s 47ms/step - loss: 0.0017\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 5s 45ms/step - loss: 0.0016\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 6s 48ms/step - loss: 0.0014\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 5s 47ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 5s 47ms/step - loss: 0.0012\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 5s 47ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 6s 49ms/step - loss: 0.0011\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 6s 50ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 6s 51ms/step - loss: 0.0012\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 6s 48ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 6s 49ms/step - loss: 0.0013\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 5s 46ms/step - loss: 0.0012\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0013\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0016\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 7.3801e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.5259e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.6088e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.1802e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.5032e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.8719e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.0805e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.1823e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.3407e-04\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 11s 75ms/step - loss: 0.0026\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0023\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0015\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0016\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0013\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 8s 73ms/step - loss: 0.0012\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0013\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0018\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 9s 77ms/step - loss: 0.0016\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 8s 73ms/step - loss: 0.0018\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0019\n",
            "Epoch 00012: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 10s 62ms/step - loss: 0.0027\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0019\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0014\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.0019\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0013\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0011\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0012\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 0.0012\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 7s 65ms/step - loss: 0.0016\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0015\n",
            "Epoch 00012: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0083\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0088\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0048\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.8192e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.5931e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.2695e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.0446e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.1974e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.6407e-04\n",
            "Epoch 00013: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0020\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0037\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0019\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 00011: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 11s 72ms/step - loss: 0.0026\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0015\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0019\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0014\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0015\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0015\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0017\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 8s 70ms/step - loss: 0.0017\n",
            "Epoch 00010: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 10s 62ms/step - loss: 0.0031\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 0.0017\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0014\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0012\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0016\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0012\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0013\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0011\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0012\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0010\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 9.4909e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0010\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0012\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0012\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0013\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0013\n",
            "Epoch 00020: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0048\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0083\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0043\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.5893e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.6491e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.4949e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.5515e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.0433e-04\n",
            "Epoch 00010: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0017\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0031\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0020\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.8772e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.3818e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.9613e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.1916e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.2768e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.2033e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.1478e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.0785e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.9801e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.9842e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.9878e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.0208e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0795e-04\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0860e-04\n",
            "Epoch 00019: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 11s 73ms/step - loss: 0.0025\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0032\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0022\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 9s 74ms/step - loss: 0.0022\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0022\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0022\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0023\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0025\n",
            "Epoch 00008: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 10s 64ms/step - loss: 0.0035\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0022\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0019\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0018\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 0.0013\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0012\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0011\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0013\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0011\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0010\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0011\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0013\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0012\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0013\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 0.0012\n",
            "Epoch 00017: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0012\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.0971e-04\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.2989e-04\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.3878e-04\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.2083e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.3165e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.4103e-04\n",
            "Epoch 00008: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0027\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0047\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0036\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0021\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0020\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0021\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0033\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0038\n",
            "Epoch 00013: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 11s 72ms/step - loss: 0.0038\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0025\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0020\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0019\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 9s 77ms/step - loss: 0.0021\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0022\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0019\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0018\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 8s 73ms/step - loss: 0.0016\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 8s 70ms/step - loss: 0.0015\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 8s 73ms/step - loss: 0.0019\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0020\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0022\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0028\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0028\n",
            "Epoch 00015: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 10s 63ms/step - loss: 0.0030\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0017\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0019\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.0013\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.0014\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0013\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 0.0013\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0011\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0015\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0013\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0015\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0014\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0012\n",
            "Epoch 00015: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0215\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0120\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0049\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0010\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.6173e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.5216e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.8394e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0010\n",
            "Epoch 00010: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 7.8808e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.6433e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.0168e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.8810e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.9416e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.2475e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.4020e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.4482e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.4321e-04\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 11s 73ms/step - loss: 0.0029\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 8s 73ms/step - loss: 0.0025\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0016\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 8s 73ms/step - loss: 0.0014\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0015\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 8s 73ms/step - loss: 0.0016\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0016\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0018\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0018\n",
            "Epoch 00009: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 10s 62ms/step - loss: 0.0028\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0020\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0019\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0013\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0012\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0013\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 0.0011\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0011\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0011\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 9.2658e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0012\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 7s 61ms/step - loss: 0.0011\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0011\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 7s 62ms/step - loss: 0.0012\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 7s 63ms/step - loss: 0.0011\n",
            "Epoch 00017: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0109\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0060\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0049\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 00009: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0014\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0011\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.4705e-04\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.2723e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 7.3843e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 7.0926e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.6819e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.5922e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.5428e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.6506e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.7173e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.7798e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.7746e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.7192e-04\n",
            "Epoch 00016: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 12s 82ms/step - loss: 0.0026\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0022\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.0018\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0017\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0017\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0016\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 9s 82ms/step - loss: 0.0014\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 9s 82ms/step - loss: 0.0014\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0015\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.0017\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.0015\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.0016\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.0016\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 11s 71ms/step - loss: 0.0036\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0020\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0019\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 8s 70ms/step - loss: 0.0017\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0015\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0013\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 8s 70ms/step - loss: 0.0015\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 8s 70ms/step - loss: 0.0013\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0010\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0011\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 8s 70ms/step - loss: 0.0010\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0011\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 9s 75ms/step - loss: 0.0011\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0012\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0012\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 8s 70ms/step - loss: 9.5842e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0011\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 8s 70ms/step - loss: 0.0011\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0013\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0013\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0016\n",
            "Epoch 00022: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0146\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0104\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0058\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0010\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.8759e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.6097e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.4509e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.3423e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.2582e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.1811e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.1143e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.0596e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.0093e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.9528e-04\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.8710e-04\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.7663e-04\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.6395e-04\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.4937e-04\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.3314e-04\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.1575e-04\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.9750e-04\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.7899e-04\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.5997e-04\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.4079e-04\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.2179e-04\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.0242e-04\n",
            "Epoch 31/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.8047e-04\n",
            "Epoch 32/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.6044e-04\n",
            "Epoch 33/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.4284e-04\n",
            "Epoch 34/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.2814e-04\n",
            "Epoch 35/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.1432e-04\n",
            "Epoch 36/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.0371e-04\n",
            "Epoch 37/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.9467e-04\n",
            "Epoch 38/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.8696e-04\n",
            "Epoch 39/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.7841e-04\n",
            "Epoch 40/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.7176e-04\n",
            "Epoch 41/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.6498e-04\n",
            "Epoch 42/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.5804e-04\n",
            "Epoch 43/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.5099e-04\n",
            "Epoch 44/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.4388e-04\n",
            "Epoch 45/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.3695e-04\n",
            "Epoch 46/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.2749e-04\n",
            "Epoch 47/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.2056e-04\n",
            "Epoch 48/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.1292e-04\n",
            "Epoch 49/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0340e-04\n",
            "Epoch 50/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.9456e-04\n",
            "Epoch 51/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.8506e-04\n",
            "Epoch 52/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.7605e-04\n",
            "Epoch 53/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.6760e-04\n",
            "Epoch 54/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.5663e-04\n",
            "Epoch 55/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.4854e-04\n",
            "Epoch 56/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.4054e-04\n",
            "Epoch 57/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.3215e-04\n",
            "Epoch 58/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.2353e-04\n",
            "Epoch 59/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.1509e-04\n",
            "Epoch 60/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.0693e-04\n",
            "Epoch 61/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.9993e-04\n",
            "Epoch 62/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.9132e-04\n",
            "Epoch 63/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.8347e-04\n",
            "Epoch 64/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.7499e-04\n",
            "Epoch 65/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.6730e-04\n",
            "Epoch 66/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.6050e-04\n",
            "Epoch 67/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.5239e-04\n",
            "Epoch 68/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.4531e-04\n",
            "Epoch 69/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.3843e-04\n",
            "Epoch 70/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.3128e-04\n",
            "Epoch 71/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.2370e-04\n",
            "Epoch 72/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.1573e-04\n",
            "Epoch 73/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.0963e-04\n",
            "Epoch 74/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.0238e-04\n",
            "Epoch 75/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.9581e-04\n",
            "Epoch 76/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.8853e-04\n",
            "Epoch 77/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.8123e-04\n",
            "Epoch 78/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.7426e-04\n",
            "Epoch 79/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.6749e-04\n",
            "Epoch 80/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.6069e-04\n",
            "Epoch 81/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.5407e-04\n",
            "Epoch 82/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.4781e-04\n",
            "Epoch 83/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.4217e-04\n",
            "Epoch 84/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.3711e-04\n",
            "Epoch 85/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.3282e-04\n",
            "Epoch 86/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.2969e-04\n",
            "Epoch 87/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.2737e-04\n",
            "Epoch 88/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.2607e-04\n",
            "Epoch 89/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.2577e-04\n",
            "Epoch 90/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.2643e-04\n",
            "Epoch 91/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.2751e-04\n",
            "Epoch 00091: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0031\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0041\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0035\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.8845e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.8959e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 7.7068e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 7.1143e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 6.6070e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 6.2757e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.9673e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.6193e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.3465e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.0202e-04\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.7527e-04\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.4500e-04\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.1113e-04\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.8677e-04\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.5493e-04\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.3606e-04\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.2111e-04\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.1083e-04\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.0493e-04\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.9835e-04\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 2.9595e-04\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.9092e-04\n",
            "Epoch 31/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 2.9350e-04\n",
            "Epoch 32/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.0605e-04\n",
            "Epoch 33/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.1611e-04\n",
            "Epoch 34/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.2875e-04\n",
            "Epoch 35/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.3998e-04\n",
            "Epoch 00035: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 12s 83ms/step - loss: 0.0028\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0017\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0020\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.0017\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.0016\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0019\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.0016\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.0018\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.0014\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.0015\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.0017\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.0015\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0015\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0015\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 11s 71ms/step - loss: 0.0025\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0021\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0017\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 8s 73ms/step - loss: 0.0014\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0014\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0011\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0012\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0011\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0012\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0010\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0011\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 9.0782e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0011\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 9.8819e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 9.5123e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 9s 75ms/step - loss: 0.0011\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 9s 73ms/step - loss: 0.0011\n",
            "Epoch 00019: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0031\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0071\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0069\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0018\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0021\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.4305e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.8201e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.5560e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.8790e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.8621e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.0778e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 9.4092e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.5749e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.9198e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 00018: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 12s 82ms/step - loss: 0.0029\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0030\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0029\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0024\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0024\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0020\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0022\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.0026\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.0025\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.0028\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.0028\n",
            "Epoch 00011: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 12s 71ms/step - loss: 0.0028\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0017\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0018\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 9s 73ms/step - loss: 0.0014\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0012\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0013\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0015\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0012\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0012\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0013\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 9s 74ms/step - loss: 0.0012\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 8s 73ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 9s 73ms/step - loss: 0.0011\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 8s 73ms/step - loss: 0.0013\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0013\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0014\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 8s 73ms/step - loss: 0.0018\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0020\n",
            "Epoch 00018: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0019\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0013\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.0906e-04\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.4267e-04\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.0524e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.4930e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.6626e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.1351e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.8904e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 6.3216e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.9030e-04\n",
            "Epoch 00012: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 12s 85ms/step - loss: 0.0028\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.0032\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.0027\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.0030\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0021\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0022\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0022\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 9s 82ms/step - loss: 0.0023\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0023\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0024\n",
            "Epoch 00010: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 11s 70ms/step - loss: 0.0030\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 8s 70ms/step - loss: 0.0026\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 8s 70ms/step - loss: 0.0020\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 8s 70ms/step - loss: 0.0016\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 8s 69ms/step - loss: 0.0015\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0016\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0015\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 8s 70ms/step - loss: 0.0013\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0015\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0015\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 8s 70ms/step - loss: 0.0015\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0014\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 8s 70ms/step - loss: 0.0014\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0150\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0061\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0040\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0019\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 12s 82ms/step - loss: 0.0027\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.0026\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0022\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0024\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0025\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0022\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 9s 82ms/step - loss: 0.0020\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.0023\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.0021\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.0025\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.0027\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.0026\n",
            "Epoch 00012: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 11s 69ms/step - loss: 0.0025\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 8s 70ms/step - loss: 0.0017\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0017\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 8s 71ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0013\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 9s 73ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0015\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 9s 74ms/step - loss: 0.0012\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0011\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 8s 73ms/step - loss: 0.0012\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 8s 73ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 8s 73ms/step - loss: 0.0013\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 8s 72ms/step - loss: 0.0014\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0021\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0016\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0019\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0016\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0011\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.8885e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 8.7999e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 8.2685e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.0043e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.7157e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 7.3403e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 7.5299e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 7.3429e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 7.6044e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 7.7603e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.2378e-04\n",
            "Epoch 00016: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 14s 95ms/step - loss: 0.0032\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0020\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0020\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0019\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 11s 93ms/step - loss: 0.0022\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 11s 93ms/step - loss: 0.0019\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 11s 91ms/step - loss: 0.0021\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 11s 92ms/step - loss: 0.0019\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 11s 92ms/step - loss: 0.0019\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 11s 93ms/step - loss: 0.0020\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 11s 92ms/step - loss: 0.0022\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 11s 93ms/step - loss: 0.0023\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 11s 93ms/step - loss: 0.0027\n",
            "Epoch 00013: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 12s 78ms/step - loss: 0.0026\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0022\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0013\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0013\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0012\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0011\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0013\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 9s 78ms/step - loss: 0.0013\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 9s 77ms/step - loss: 0.0011\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0011\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0013\n",
            "Epoch 00012: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0020\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0016\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0021\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.3945e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.0730e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 7.5077e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 7.2379e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 7.3198e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.3676e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 7.7290e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.1641e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.7924e-04\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 14s 92ms/step - loss: 0.0031\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 11s 94ms/step - loss: 0.0032\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 11s 94ms/step - loss: 0.0024\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 11s 92ms/step - loss: 0.0020\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 11s 93ms/step - loss: 0.0018\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 11s 92ms/step - loss: 0.0019\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 11s 91ms/step - loss: 0.0019\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 11s 93ms/step - loss: 0.0021\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 11s 93ms/step - loss: 0.0020\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 11s 93ms/step - loss: 0.0019\n",
            "Epoch 00010: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 12s 80ms/step - loss: 0.0027\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0019\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0025\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0016\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0016\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0013\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0013\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0013\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0010\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0012\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0013\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0011\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 9.1766e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0010\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 9.6223e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0012\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0015\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0016\n",
            "Epoch 00019: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0029\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0048\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0059\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0013\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0018\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0017\n",
            "Epoch 00006: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 14s 95ms/step - loss: 0.0029\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0019\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0014\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0015\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0013\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0014\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0016\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0016\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0015\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 11s 93ms/step - loss: 0.0017\n",
            "Epoch 00011: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 12s 79ms/step - loss: 0.0033\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0020\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0020\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0015\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 9s 78ms/step - loss: 0.0013\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 9s 78ms/step - loss: 0.0013\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0012\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.0011\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.0011\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 9.2308e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0011\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 9.3423e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.0010\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 9.2275e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 7.9826e-04\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 8.3675e-04\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 8.8404e-04\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 7.5650e-04\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.0011\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 8.4327e-04\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 7.0643e-04\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 7.5926e-04\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 7.5712e-04\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 8.4862e-04\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 8.8203e-04\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0010\n",
            "Epoch 00029: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0020\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0022\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0036\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 14s 96ms/step - loss: 0.0037\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0019\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 11s 94ms/step - loss: 0.0017\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0014\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0017\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0018\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0022\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 11s 94ms/step - loss: 0.0017\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0021\n",
            "Epoch 00009: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 13s 80ms/step - loss: 0.0036\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0021\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0025\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0017\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 9s 78ms/step - loss: 0.0015\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0013\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0014\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 9s 78ms/step - loss: 0.0012\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0013\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0013\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0011\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0013\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0014\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 9s 78ms/step - loss: 0.0016\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0016\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0015\n",
            "Epoch 00019: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0055\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0050\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0017\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0023\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0018\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0016\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.2063e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 9.2559e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 8.9743e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0017\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0014\n",
            "Epoch 00015: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 14s 95ms/step - loss: 0.0029\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 11s 94ms/step - loss: 0.0032\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0032\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0025\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0026\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0029\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0030\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0033\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0038\n",
            "Epoch 00009: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 13s 80ms/step - loss: 0.0032\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0020\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0014\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0014\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0012\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0014\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0012\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0011\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.0011\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 0.0011\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.0010\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 9.7720e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 9s 80ms/step - loss: 9.9307e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0013\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0013\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 9s 79ms/step - loss: 0.0012\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.0012\n",
            "Epoch 00021: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0017\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.6846e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.3312e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.0505e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.7161e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.3851e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.0243e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.6985e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.3866e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.0987e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.7918e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.4888e-04\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.1621e-04\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.8327e-04\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.4736e-04\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0916e-04\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.6768e-04\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.2573e-04\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 3.8382e-04\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.4530e-04\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.1368e-04\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.9067e-04\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.7736e-04\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.7181e-04\n",
            "Epoch 31/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.7197e-04\n",
            "Epoch 32/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.7638e-04\n",
            "Epoch 00032: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0034\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0073\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0065\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0041\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0032\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 1s 6ms/step - loss: 0.0024\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0022\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0027\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0026\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0028\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0031\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 00013: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 15s 105ms/step - loss: 0.0039\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 12s 107ms/step - loss: 0.0020\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 12s 107ms/step - loss: 0.0022\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 12s 105ms/step - loss: 0.0017\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 12s 105ms/step - loss: 0.0019\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 12s 105ms/step - loss: 0.0019\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 12s 105ms/step - loss: 0.0020\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 12s 106ms/step - loss: 0.0020\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 12s 104ms/step - loss: 0.0021\n",
            "Epoch 00009: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 13s 87ms/step - loss: 0.0026\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.0017\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.0017\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.0013\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0011\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.0012\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0013\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.0013\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.0010\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.0011\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.0014\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.0012\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0012\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.3984e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.6051e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.2801e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.1576e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.9434e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.4266e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.2828e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.8595e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.9685e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.5591e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.2388e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.9894e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.8758e-04\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.9216e-04\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.1521e-04\n",
            "Epoch 00020: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0011\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.1595e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.8323e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.4437e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 8.4567e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 8.4245e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.5251e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 9.1806e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 9.0951e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.7729e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.8725e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.6535e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.3635e-04\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 3.1190e-04\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.0122e-04\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 2.9626e-04\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 2.9184e-04\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 2.9048e-04\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 2.8981e-04\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.1457e-04\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.0210e-04\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.1568e-04\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.3527e-04\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.6708e-04\n",
            "Epoch 00029: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 15s 105ms/step - loss: 0.0030\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 12s 103ms/step - loss: 0.0021\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 12s 101ms/step - loss: 0.0022\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 12s 104ms/step - loss: 0.0019\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 12s 102ms/step - loss: 0.0019\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 12s 104ms/step - loss: 0.0018\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 12s 101ms/step - loss: 0.0020\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 12s 103ms/step - loss: 0.0018\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 12s 103ms/step - loss: 0.0017\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 12s 102ms/step - loss: 0.0017\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 12s 102ms/step - loss: 0.0015\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 12s 102ms/step - loss: 0.0015\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 12s 102ms/step - loss: 0.0017\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 12s 103ms/step - loss: 0.0013\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 12s 103ms/step - loss: 0.0015\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 13s 108ms/step - loss: 0.0016\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 12s 106ms/step - loss: 0.0016\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 12s 103ms/step - loss: 0.0018\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 12s 103ms/step - loss: 0.0019\n",
            "Epoch 00019: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 13s 86ms/step - loss: 0.0026\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.0020\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 10s 89ms/step - loss: 0.0015\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 10s 90ms/step - loss: 0.0017\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 10s 89ms/step - loss: 0.0015\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 10s 89ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 10s 90ms/step - loss: 0.0012\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 11s 91ms/step - loss: 0.0012\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 10s 90ms/step - loss: 0.0013\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 11s 93ms/step - loss: 0.0013\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 11s 91ms/step - loss: 0.0014\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 11s 90ms/step - loss: 0.0014\n",
            "Epoch 00012: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0020\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0010\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.0571e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 8.1274e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.6946e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.4424e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.9950e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.5574e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.1794e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.8707e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.5932e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.3439e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0277e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.6220e-04\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.2714e-04\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.9141e-04\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.6762e-04\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.5616e-04\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.4731e-04\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.4404e-04\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.4573e-04\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.5105e-04\n",
            "Epoch 00026: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0018\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0026\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0014\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0011\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 9.6686e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.6486e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 8.6071e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 8.8571e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 9.3299e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 9.7270e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 9.5259e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 8.9508e-04\n",
            "Epoch 00013: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 15s 109ms/step - loss: 0.0034\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 13s 108ms/step - loss: 0.0025\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 13s 108ms/step - loss: 0.0022\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 13s 113ms/step - loss: 0.0021\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 13s 108ms/step - loss: 0.0017\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 12s 107ms/step - loss: 0.0020\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 12s 105ms/step - loss: 0.0020\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 12s 106ms/step - loss: 0.0021\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 12s 106ms/step - loss: 0.0021\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 12s 108ms/step - loss: 0.0019\n",
            "Epoch 00010: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 23s 175ms/step - loss: 0.0029\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0020\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0020\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 10s 89ms/step - loss: 0.0018\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 10s 90ms/step - loss: 0.0018\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 10s 90ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 11s 90ms/step - loss: 0.0011\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 11s 91ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 10s 89ms/step - loss: 0.0013\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0013\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.0013\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0011\n",
            "Epoch 00012: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0022\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0041\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0018\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.8019e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.4440e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.7535e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 00012: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 15s 105ms/step - loss: 0.0038\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 12s 105ms/step - loss: 0.0023\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 12s 103ms/step - loss: 0.0021\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 12s 105ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 13s 108ms/step - loss: 0.0016\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 13s 108ms/step - loss: 0.0020\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 12s 105ms/step - loss: 0.0017\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 12s 107ms/step - loss: 0.0017\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 12s 106ms/step - loss: 0.0020\n",
            "Epoch 00009: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 13s 86ms/step - loss: 0.0027\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0018\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.0018\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.0020\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0013\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.0012\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 9.6548e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 10s 89ms/step - loss: 0.0011\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.0012\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.0010\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.0010\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0010\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0011\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 9.0182e-04\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.9304e-04\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 7.3610e-04\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.8043e-04\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.4943e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.3569e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.3143e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.2697e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.1639e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.1021e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 6.0353e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.9618e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.8566e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.7460e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.6370e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.4987e-04\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.3518e-04\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.1906e-04\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 5.0187e-04\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.8268e-04\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.6400e-04\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.4290e-04\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.2227e-04\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 4.0049e-04\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.8019e-04\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.6102e-04\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.4394e-04\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.3269e-04\n",
            "Epoch 31/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.2166e-04\n",
            "Epoch 32/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.1528e-04\n",
            "Epoch 33/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.0835e-04\n",
            "Epoch 34/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.0507e-04\n",
            "Epoch 35/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.0226e-04\n",
            "Epoch 36/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 3.0097e-04\n",
            "Epoch 37/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.9816e-04\n",
            "Epoch 38/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.9666e-04\n",
            "Epoch 39/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.9569e-04\n",
            "Epoch 40/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.9361e-04\n",
            "Epoch 41/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.9239e-04\n",
            "Epoch 42/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.9065e-04\n",
            "Epoch 43/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.8748e-04\n",
            "Epoch 44/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.8716e-04\n",
            "Epoch 45/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.8398e-04\n",
            "Epoch 46/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.7996e-04\n",
            "Epoch 47/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.7740e-04\n",
            "Epoch 48/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.7611e-04\n",
            "Epoch 49/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.7507e-04\n",
            "Epoch 50/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.7832e-04\n",
            "Epoch 51/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 2.8292e-04\n",
            "Epoch 00051: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0015\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0020\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0017\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0013\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.3748e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.0874e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.9422e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.4028e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0010\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 00016: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 15s 106ms/step - loss: 0.0037\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 12s 106ms/step - loss: 0.0029\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 12s 106ms/step - loss: 0.0027\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 12s 105ms/step - loss: 0.0024\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 12s 105ms/step - loss: 0.0025\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 12s 107ms/step - loss: 0.0023\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 12s 107ms/step - loss: 0.0022\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 12s 106ms/step - loss: 0.0023\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 12s 105ms/step - loss: 0.0025\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 12s 106ms/step - loss: 0.0030\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 12s 105ms/step - loss: 0.0039\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 12s 105ms/step - loss: 0.0036\n",
            "Epoch 00012: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 13s 88ms/step - loss: 0.0029\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.0019\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0026\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.0017\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 10s 89ms/step - loss: 0.0017\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 10s 89ms/step - loss: 0.0015\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 10s 89ms/step - loss: 0.0013\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 10s 90ms/step - loss: 0.0011\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 10s 90ms/step - loss: 0.0011\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 10s 89ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0011\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.0012\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 10s 89ms/step - loss: 0.0013\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0013\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 10s 90ms/step - loss: 0.0013\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.0013\n",
            "Epoch 00018: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0012\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 00008: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0020\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0021\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0017\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.3850e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 8.7634e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 8.2377e-04\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 7.8181e-04\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 7.1765e-04\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 6.3932e-04\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 5.5213e-04\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.8296e-04\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.5462e-04\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.4612e-04\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.4255e-04\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 4.4647e-04\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 4.4301e-04\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 4.4705e-04\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 4.4374e-04\n",
            "Epoch 31/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 4.4362e-04\n",
            "Epoch 00031: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 16s 116ms/step - loss: 0.0031\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 14s 118ms/step - loss: 0.0025\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 14s 118ms/step - loss: 0.0019\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 13s 115ms/step - loss: 0.0017\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 13s 115ms/step - loss: 0.0019\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 14s 116ms/step - loss: 0.0017\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 13s 116ms/step - loss: 0.0018\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 13s 116ms/step - loss: 0.0019\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 13s 114ms/step - loss: 0.0020\n",
            "Epoch 00009: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 14s 94ms/step - loss: 0.0025\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 11s 94ms/step - loss: 0.0017\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 11s 94ms/step - loss: 0.0016\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 11s 94ms/step - loss: 0.0023\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 11s 94ms/step - loss: 0.0015\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0013\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0013\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 11s 94ms/step - loss: 9.8934e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0010\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 11s 94ms/step - loss: 0.0010\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 11s 94ms/step - loss: 0.0010\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0020\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0013\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0015\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 00006: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 16s 115ms/step - loss: 0.0027\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 13s 116ms/step - loss: 0.0023\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 13s 115ms/step - loss: 0.0017\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 13s 116ms/step - loss: 0.0017\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 14s 118ms/step - loss: 0.0017\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 13s 115ms/step - loss: 0.0015\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 14s 118ms/step - loss: 0.0015\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 14s 119ms/step - loss: 0.0017\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 14s 117ms/step - loss: 0.0019\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 14s 123ms/step - loss: 0.0016\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 14s 121ms/step - loss: 0.0017\n",
            "Epoch 00011: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 14s 98ms/step - loss: 0.0029\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 0.0022\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 12s 100ms/step - loss: 0.0014\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 12s 101ms/step - loss: 0.0014\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 12s 99ms/step - loss: 0.0015\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0012\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0012\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0014\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0017\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0010\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0011\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0011\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0010\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0011\n",
            "Epoch 00016: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0018\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0023\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 1s 6ms/step - loss: 0.0017\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0014\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0011\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 9.4845e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 8.6622e-04\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.7596e-04\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 8.8839e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.0296e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 9.2785e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 9.3890e-04\n",
            "Epoch 00013: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 16s 119ms/step - loss: 0.0021\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 14s 120ms/step - loss: 0.0017\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 14s 120ms/step - loss: 0.0025\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 14s 118ms/step - loss: 0.0015\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 14s 120ms/step - loss: 0.0013\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 14s 119ms/step - loss: 0.0011\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 14s 117ms/step - loss: 0.0014\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 14s 117ms/step - loss: 0.0017\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 14s 117ms/step - loss: 0.0019\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 13s 116ms/step - loss: 0.0019\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 14s 119ms/step - loss: 0.0021\n",
            "Epoch 00011: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 15s 99ms/step - loss: 0.0024\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0016\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0018\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 0.0014\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 11s 99ms/step - loss: 0.0013\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0015\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 0.0012\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 0.0014\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 0.0016\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 12s 99ms/step - loss: 0.0011\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 11s 99ms/step - loss: 0.0011\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 9.6507e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 11s 99ms/step - loss: 0.0011\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0011\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0012\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 0.0012\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0011\n",
            "Epoch 00018: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0026\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0023\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0030\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0027\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0011\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0011\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0011\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0010\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 1s 6ms/step - loss: 9.7853e-04\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 9.4820e-04\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 9.1821e-04\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 8.7002e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 8.3072e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 7.7571e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 7.3447e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 7.2098e-04\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 7.1438e-04\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 7.1997e-04\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 7.2427e-04\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 7.2141e-04\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 7.0991e-04\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.9212e-04\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 6.6858e-04\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 6.4462e-04\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.2134e-04\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.9957e-04\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.8464e-04\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.7211e-04\n",
            "Epoch 31/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 5.6026e-04\n",
            "Epoch 32/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 5.4351e-04\n",
            "Epoch 33/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 5.2613e-04\n",
            "Epoch 34/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 4.8973e-04\n",
            "Epoch 35/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.7529e-04\n",
            "Epoch 36/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 4.5818e-04\n",
            "Epoch 37/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 4.4816e-04\n",
            "Epoch 38/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 4.3367e-04\n",
            "Epoch 39/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 4.1605e-04\n",
            "Epoch 40/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 3.9704e-04\n",
            "Epoch 41/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 3.8502e-04\n",
            "Epoch 42/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 3.6680e-04\n",
            "Epoch 43/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 3.4996e-04\n",
            "Epoch 44/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 3.4127e-04\n",
            "Epoch 45/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 3.3004e-04\n",
            "Epoch 46/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 3.2598e-04\n",
            "Epoch 47/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 3.4634e-04\n",
            "Epoch 48/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 3.4290e-04\n",
            "Epoch 49/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 3.2167e-04\n",
            "Epoch 50/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 3.1862e-04\n",
            "Epoch 51/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 3.0704e-04\n",
            "Epoch 52/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 3.2047e-04\n",
            "Epoch 53/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 2.9989e-04\n",
            "Epoch 54/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.3326e-04\n",
            "Epoch 55/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 2.9006e-04\n",
            "Epoch 56/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.3585e-04\n",
            "Epoch 57/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 2.7563e-04\n",
            "Epoch 58/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 3.1958e-04\n",
            "Epoch 59/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 2.7626e-04\n",
            "Epoch 60/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.3522e-04\n",
            "Epoch 61/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 2.8145e-04\n",
            "Epoch 62/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 3.6010e-04\n",
            "Epoch 00062: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 16s 117ms/step - loss: 0.0041\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 14s 116ms/step - loss: 0.0020\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 14s 117ms/step - loss: 0.0021\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 14s 118ms/step - loss: 0.0020\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 13s 116ms/step - loss: 0.0023\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 13s 114ms/step - loss: 0.0020\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 14s 120ms/step - loss: 0.0019\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 14s 118ms/step - loss: 0.0018\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 14s 118ms/step - loss: 0.0020\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 14s 116ms/step - loss: 0.0020\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 14s 117ms/step - loss: 0.0021\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 14s 121ms/step - loss: 0.0023\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 14s 117ms/step - loss: 0.0030\n",
            "Epoch 00013: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 14s 97ms/step - loss: 0.0025\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0020\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 0.0020\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0017\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 0.0013\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0016\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0015\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0015\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 0.0013\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0012\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.0012\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 8.7553e-04\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0010\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 9.9930e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 9.8093e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 9.4528e-04\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 9.1729e-04\n",
            "Epoch 00019: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0026\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0019\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0017\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 8.7632e-04\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 9.4561e-04\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0010\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 8.9735e-04\n",
            "Epoch 00012: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 17s 120ms/step - loss: 0.0034\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 13s 116ms/step - loss: 0.0026\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 14s 119ms/step - loss: 0.0024\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 14s 116ms/step - loss: 0.0021\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 14s 117ms/step - loss: 0.0020\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 14s 117ms/step - loss: 0.0023\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 14s 117ms/step - loss: 0.0022\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 14s 116ms/step - loss: 0.0019\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 13s 116ms/step - loss: 0.0019\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 13s 114ms/step - loss: 0.0023\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 13s 114ms/step - loss: 0.0023\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 13s 114ms/step - loss: 0.0021\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 13s 114ms/step - loss: 0.0027\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 13s 113ms/step - loss: 0.0025\n",
            "Epoch 00014: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 14s 97ms/step - loss: 0.0025\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0018\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0019\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0015\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0013\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 0.0014\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0012\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 0.0013\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 0.0012\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0011\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 0.0011\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 0.0011\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0010\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 9.9872e-04\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 9.6013e-04\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 12s 100ms/step - loss: 9.4831e-04\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 11s 96ms/step - loss: 0.0011\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0011\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 11s 98ms/step - loss: 0.0015\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0013\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 11s 97ms/step - loss: 0.0013\n",
            "Epoch 00022: early stopping\n",
            "data loaded for AXISBANK!!\n",
            "             Close\n",
            "Date              \n",
            "2002-01-01   26.30\n",
            "2002-01-02   26.05\n",
            "2002-01-03   27.20\n",
            "2002-01-04   27.20\n",
            "2002-01-07   27.80\n",
            "...            ...\n",
            "2019-01-09  670.10\n",
            "2019-01-10  663.25\n",
            "2019-01-11  666.50\n",
            "2019-01-14  659.20\n",
            "2019-01-15  660.60\n",
            "\n",
            "[4245 rows x 1 columns]\n",
            "Epoch 1/200\n",
            "117/117 [==============================] - 1s 3ms/step - loss: 0.0045\n",
            "Epoch 2/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 3/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 4/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 5/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 7.9906e-04\n",
            "Epoch 6/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 6.7162e-04\n",
            "Epoch 7/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 6.2467e-04\n",
            "Epoch 8/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 6.0953e-04\n",
            "Epoch 9/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 6.0533e-04\n",
            "Epoch 10/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 6.0385e-04\n",
            "Epoch 11/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 6.0290e-04\n",
            "Epoch 12/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 5.9943e-04\n",
            "Epoch 13/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 5.9332e-04\n",
            "Epoch 14/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 5.8347e-04\n",
            "Epoch 15/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 5.6787e-04\n",
            "Epoch 16/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 5.5029e-04\n",
            "Epoch 17/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 5.3121e-04\n",
            "Epoch 18/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 5.1288e-04\n",
            "Epoch 19/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.9626e-04\n",
            "Epoch 20/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.8126e-04\n",
            "Epoch 21/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.6751e-04\n",
            "Epoch 22/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.5536e-04\n",
            "Epoch 23/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.4415e-04\n",
            "Epoch 24/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.3409e-04\n",
            "Epoch 25/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.2674e-04\n",
            "Epoch 26/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.2116e-04\n",
            "Epoch 27/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.1489e-04\n",
            "Epoch 28/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.0833e-04\n",
            "Epoch 29/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.0209e-04\n",
            "Epoch 30/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.9689e-04\n",
            "Epoch 31/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.9106e-04\n",
            "Epoch 32/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.8561e-04\n",
            "Epoch 33/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.8025e-04\n",
            "Epoch 34/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.7481e-04\n",
            "Epoch 35/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.7219e-04\n",
            "Epoch 36/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.6652e-04\n",
            "Epoch 37/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.6214e-04\n",
            "Epoch 38/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.6007e-04\n",
            "Epoch 39/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.5510e-04\n",
            "Epoch 40/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.5383e-04\n",
            "Epoch 41/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.4890e-04\n",
            "Epoch 42/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.4960e-04\n",
            "Epoch 43/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 3.4325e-04\n",
            "Epoch 44/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.4603e-04\n",
            "Epoch 45/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.3891e-04\n",
            "Epoch 46/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.4551e-04\n",
            "Epoch 47/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.3375e-04\n",
            "Epoch 48/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.3911e-04\n",
            "Epoch 49/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.3416e-04\n",
            "Epoch 00049: early stopping\n",
            "Epoch 1/200\n",
            "117/117 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 2/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 3/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 4/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 8.6311e-04\n",
            "Epoch 5/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 6.7941e-04\n",
            "Epoch 6/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 6.3888e-04\n",
            "Epoch 7/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 6.0902e-04\n",
            "Epoch 8/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 5.7757e-04\n",
            "Epoch 9/200\n",
            "117/117 [==============================] - 0s 4ms/step - loss: 5.5585e-04\n",
            "Epoch 10/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 5.3740e-04\n",
            "Epoch 11/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 5.2006e-04\n",
            "Epoch 12/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 5.2922e-04\n",
            "Epoch 13/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 5.0926e-04\n",
            "Epoch 14/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 5.1197e-04\n",
            "Epoch 15/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.9448e-04\n",
            "Epoch 16/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.9281e-04\n",
            "Epoch 17/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.8185e-04\n",
            "Epoch 18/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 5.0537e-04\n",
            "Epoch 19/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.5180e-04\n",
            "Epoch 20/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.6608e-04\n",
            "Epoch 21/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.5142e-04\n",
            "Epoch 22/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.3861e-04\n",
            "Epoch 23/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.4029e-04\n",
            "Epoch 24/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.8461e-04\n",
            "Epoch 25/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.4073e-04\n",
            "Epoch 26/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.5881e-04\n",
            "Epoch 27/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.1110e-04\n",
            "Epoch 28/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.6337e-04\n",
            "Epoch 29/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.6097e-04\n",
            "Epoch 30/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.0694e-04\n",
            "Epoch 31/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.3639e-04\n",
            "Epoch 32/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.0309e-04\n",
            "Epoch 33/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.1219e-04\n",
            "Epoch 34/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.9601e-04\n",
            "Epoch 35/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.8489e-04\n",
            "Epoch 36/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.9625e-04\n",
            "Epoch 37/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.0366e-04\n",
            "Epoch 38/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.7556e-04\n",
            "Epoch 39/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 5.9153e-04\n",
            "Epoch 40/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 4.1844e-04\n",
            "Epoch 41/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.9617e-04\n",
            "Epoch 42/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.9938e-04\n",
            "Epoch 43/200\n",
            "117/117 [==============================] - 0s 3ms/step - loss: 3.8884e-04\n",
            "Epoch 00043: early stopping\n",
            "Epoch 1/200\n",
            "117/117 [==============================] - 7s 41ms/step - loss: 0.0026\n",
            "Epoch 2/200\n",
            "117/117 [==============================] - 5s 39ms/step - loss: 0.0056\n",
            "Epoch 3/200\n",
            "117/117 [==============================] - 5s 40ms/step - loss: 0.0019\n",
            "Epoch 4/200\n",
            "117/117 [==============================] - 4s 38ms/step - loss: 0.0017\n",
            "Epoch 5/200\n",
            "117/117 [==============================] - 4s 38ms/step - loss: 0.0030\n",
            "Epoch 6/200\n",
            "117/117 [==============================] - 4s 38ms/step - loss: 0.0017\n",
            "Epoch 7/200\n",
            "117/117 [==============================] - 5s 39ms/step - loss: 0.0021\n",
            "Epoch 8/200\n",
            "117/117 [==============================] - 4s 38ms/step - loss: 0.0016\n",
            "Epoch 9/200\n",
            "117/117 [==============================] - 5s 40ms/step - loss: 0.0017\n",
            "Epoch 10/200\n",
            "117/117 [==============================] - 5s 39ms/step - loss: 0.0013\n",
            "Epoch 11/200\n",
            "117/117 [==============================] - 4s 37ms/step - loss: 0.0013\n",
            "Epoch 12/200\n",
            "117/117 [==============================] - 4s 37ms/step - loss: 0.0013\n",
            "Epoch 13/200\n",
            "117/117 [==============================] - 4s 38ms/step - loss: 0.0017\n",
            "Epoch 14/200\n",
            " 85/117 [====================>.........] - ETA: 1s - loss: 8.9018e-04"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-675f64290fa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mearly_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             history_model_lstm = model.fit(X_tr_t, y_train, epochs=200, batch_size=32, verbose=1, shuffle=False,\n\u001b[0;32m---> 67\u001b[0;31m                                            callbacks=[early_stop])\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tst_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zof0S6bfyT4e",
        "outputId": "86c4c01d-0cda-469c-b42a-778765f9fb43"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import LSTM, GRU, Conv1D, MaxPooling1D\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from operator import add\n",
        "import json\n",
        "import itertools\n",
        "\n",
        "symbols = ['ACC', 'HCLTECH', 'JSWSTEEL', 'AXISBANK', 'INFY', 'HDFC', 'INFY', 'BHARTIARTL', 'ULTRACEMCO', 'CIPLA', 'MARUTI']\n",
        "\n",
        "# (x, y) here x indicates the input size and y indicates the prediction size\n",
        "# (30, 7) means we predict next 7 days stocks using previous 30 days prices\n",
        "window_size = [(30, 7), (30, 14), (60, 7), (60, 14)]\n",
        "\n",
        "\n",
        "def adj_r2_score(r2, n, k):\n",
        "    return 1 - ((1 - r2) * ((n - 1) / (n - k - 1)))\n",
        "\n",
        "\n",
        "def window_transform(series, input_size, output_size):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    slen = len(series)\n",
        "    n_windows = slen - (input_size + output_size) + 1\n",
        "\n",
        "    for i in range(n_windows):\n",
        "        X.append(series[i:i + input_size])\n",
        "        y.append(series[i+input_size: i+input_size+output_size])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    X = X.reshape(X.shape[0], X.shape[1])\n",
        "    y = y.reshape(y.shape[0], y.shape[1])\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def test_window_transform(series, input_size, output_size):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    slen = len(series)\n",
        "\n",
        "    j = input_size\n",
        "    while (j+output_size) < slen:\n",
        "        X.append(series[j - input_size:j])\n",
        "        y.append(series[j:j+output_size])\n",
        "        j = j + output_size\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    X = X.reshape(X.shape[0], X.shape[1])\n",
        "    y = y.reshape(y.shape[0], y.shape[1])\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def make_ann_model(input_size, output_size):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, activation='relu', input_shape=(input_size,)))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(output_size, activation='linear'))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "\n",
        "def make_cnn_model(input_size, output_size):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=5, strides=1, activation='relu', input_shape=(input_size, 1)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(output_size, activation='linear'))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def make_lstm_model(input_size, output_size):\n",
        "    model_lstm = Sequential()\n",
        "    model_lstm.add(LSTM(256, input_shape=(input_size, 1), activation='relu', kernel_initializer='lecun_uniform',\n",
        "                        return_sequences=True))\n",
        "    model_lstm.add(Dropout(0.2))\n",
        "    model_lstm.add(LSTM(128, input_shape=(input_size, 1), activation='relu', kernel_initializer='lecun_uniform',\n",
        "                        return_sequences=False))\n",
        "    model_lstm.add(Dropout(0.2))\n",
        "    model_lstm.add(Dense(output_size, activation='linear'))\n",
        "    opt = Adam(clipnorm=1.0)\n",
        "    model_lstm.compile(loss='mean_squared_error', optimizer=opt)\n",
        "\n",
        "    return model_lstm\n",
        "\n",
        "\n",
        "def make_gru_model(input_size, output_size):\n",
        "    model = Sequential()\n",
        "    model.add(GRU(256, input_shape=(input_size, 1), activation='relu', kernel_initializer='lecun_uniform',\n",
        "                  return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(GRU(128, input_shape=(input_size, 1), activation='relu', kernel_initializer='lecun_uniform',\n",
        "                  return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(output_size, activation='linear'))\n",
        "    opt = Adam(clipnorm=1.0)\n",
        "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def test_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    true_ = [item for sublist in y_test for item in sublist]\n",
        "    pred_ = [item for sublist in y_pred for item in sublist]\n",
        "\n",
        "    error = mse(true_, pred_)\n",
        "\n",
        "    return error, pred_\n",
        "\n",
        "\n",
        "def plot_all_model():\n",
        "    print('plotting_model')\n",
        "    ann_model = make_ann_model(3)\n",
        "    cnn_model = make_cnn_model(3)\n",
        "    lstm_model = make_lstm_model(3)\n",
        "    gru_model = make_gru_model(3)\n",
        "\n",
        "    plot_model(ann_model, to_file='ann_plot.png', show_shapes=True, show_layer_names=True)\n",
        "    plot_model(cnn_model, to_file='cnn_plot.png', show_shapes=True, show_layer_names=True)\n",
        "    plot_model(lstm_model, to_file='lstm_plot.png', show_shapes=True, show_layer_names=True)\n",
        "    plot_model(gru_model, to_file='gru_plot.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "\n",
        "\n",
        "result = []\n",
        "\n",
        "for sym in symbols:\n",
        "\n",
        "  json_descriptor = {'stock': sym, 'ann': [], 'gru': [], 'lstm': [], 'cnn': []}\n",
        "\n",
        "  data = pd.read_csv('drive/MyDrive/Findan/Sb_1/data/{}.csv'.format(sym), index_col=0)\n",
        "  print('data loaded for {}!!'.format(sym))\n",
        "  \n",
        "  data.index = pd.to_datetime(data.index)\n",
        "  data_frame = data.copy()\n",
        "  df = data_frame[[\"Close\"]]\n",
        "\n",
        "  split_date = pd.Timestamp('01-01-2017')\n",
        "\n",
        "  train = df.loc[:split_date]\n",
        "  test = df.loc[split_date:]\n",
        "\n",
        "  sc = MinMaxScaler()\n",
        "  train_sc = sc.fit_transform(train)\n",
        "  test_sc = sc.transform(test)\n",
        "\n",
        "  ann = []\n",
        "  gru = []\n",
        "  lstm = []\n",
        "  cnn = []\n",
        "\n",
        "  for in_size, out_size in window_size:\n",
        "      ann_result = []\n",
        "      gru_result = []\n",
        "      lstm_result = []\n",
        "      cnn_result = []\n",
        "      pred_ANN = []\n",
        "      pred_LSTM = []\n",
        "      pred_GRU = []\n",
        "      pred_CNN = []\n",
        "      for i in range(5):\n",
        "          # ''''''''ANN'''''''''''''''''''\n",
        "          X_train, y_train = window_transform(train_sc, in_size, out_size)\n",
        "          print('X train: ', X_train.shape)\n",
        "          print('y train: ', y_train.shape)\n",
        "          X_test, y_test = test_window_transform(test_sc, in_size, out_size)\n",
        "          flat_y_test = [item for sublist in y_test for item in sublist]\n",
        "          print('X test: ', X_test.shape)\n",
        "          print('y test: ', y_test.shape)\n",
        "          model = make_ann_model(in_size, out_size)\n",
        "          early_stop = EarlyStopping(monitor='loss', patience=2, verbose=1)\n",
        "          history = model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=1, callbacks=[early_stop],\n",
        "                              shuffle=False)\n",
        "\n",
        "          error, pred_ann = test_model(model, X_test, y_test)\n",
        "\n",
        "          ann_result.append(error)\n",
        "          # ''''''''ANN''''''''''''''''''''\n",
        "\n",
        "          # ''''''''CNN''''''''''''''''''''\n",
        "          X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "          y_train = y_train.reshape(y_train.shape[0], y_train.shape[1])\n",
        "          print('X train: ', X_train.shape)\n",
        "          print('y train: ', y_train.shape)\n",
        "          X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "          y_test = y_test.reshape(y_test.shape[0], y_test.shape[1])\n",
        "          print('X train: ', X_train.shape)\n",
        "          print('y train: ', y_train.shape)\n",
        "          model = make_cnn_model(in_size, out_size)\n",
        "          print(model.summary())\n",
        "          early_stop = EarlyStopping(monitor='loss', patience=3, verbose=1)\n",
        "          history_model_cnn = model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=1, shuffle=False,\n",
        "                                        callbacks=[early_stop])\n",
        "\n",
        "          error, pred_cnn = test_model(model, X_test, y_test)\n",
        "          cnn_result.append(error)\n",
        "          # ''''''''CNN''''''''''''''''''''\n",
        "\n",
        "          # ''''''''''LSTM''''''''''''''''''\n",
        "          model = make_lstm_model(in_size, out_size)\n",
        "          print(model.summary())\n",
        "          early_stop = EarlyStopping(monitor='loss', patience=3, verbose=1)\n",
        "          history_model_lstm = model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=1,\n",
        "                                          shuffle=False, callbacks=[early_stop])\n",
        "\n",
        "          error, pred_lstm = test_model(model, X_test, y_test)\n",
        "          lstm_result.append(error)\n",
        "          # \"''''''''LSTM'''''''''''''''''''\n",
        "\n",
        "          # ''''''''GRU'''''''''''''''''''''\n",
        "          model = make_gru_model(in_size, out_size)\n",
        "          early_stop = EarlyStopping(monitor='loss', patience=3, verbose=1)\n",
        "          history_model_gru = model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=1, shuffle=False,\n",
        "                                        callbacks=[early_stop])\n",
        "\n",
        "          error, pred_gru = test_model(model, X_test, y_test)\n",
        "          gru_result.append(error)\n",
        "          # ''''''''GRU'''''''''''''''''''''\n",
        "\n",
        "          pred_ANN.append(pred_ann)\n",
        "          pred_CNN.append(pred_cnn)\n",
        "          pred_LSTM.append(pred_lstm)\n",
        "          pred_GRU.append(pred_gru)\n",
        "      # update optimal window size param\n",
        "      ann.append([in_size, out_size, min(ann_result), np.mean(ann_result), np.std(ann_result)])\n",
        "      gru.append([in_size, out_size, min(gru_result), np.mean(gru_result), np.std(gru_result)])\n",
        "      lstm.append([in_size, out_size, min(lstm_result), np.mean(lstm_result), np.std(lstm_result)])\n",
        "      cnn.append([in_size, out_size, min(cnn_result), np.mean(cnn_result), np.std(cnn_result)])\n",
        "\n",
        "      plot_ann = [0] * len(pred_ANN[0])\n",
        "      for pred in pred_ANN:\n",
        "          plot_ann = list(map(add, plot_ann, pred))\n",
        "\n",
        "      for i in range(len(plot_ann)):\n",
        "          plot_ann[i] = plot_ann[i] / 5\n",
        "\n",
        "      plot_lstm = [0] * len(pred_LSTM[0])\n",
        "      for pred in pred_LSTM:\n",
        "          plot_lstm = list(map(add, plot_lstm, pred))\n",
        "\n",
        "      for i in range(len(plot_lstm)):\n",
        "          plot_lstm[i] = plot_lstm[i] / 5\n",
        "\n",
        "      plot_gru = [0] * len(pred_GRU[0])\n",
        "      for pred in pred_GRU:\n",
        "          plot_gru = list(map(add, plot_gru, pred))\n",
        "\n",
        "      for i in range(len(plot_gru)):\n",
        "          plot_gru[i] = plot_gru[i] / 5\n",
        "\n",
        "      plot_cnn = [0] * len(pred_CNN[0])\n",
        "      for pred in pred_CNN:\n",
        "          plot_cnn = list(map(add, plot_cnn, pred))\n",
        "\n",
        "      for i in range(len(plot_cnn)):\n",
        "          plot_cnn[i] = plot_cnn[i] / 5\n",
        "\n",
        "      # save prediction plots\n",
        "      plt.plot(flat_y_test, '-', label='True Values', color='#1b9e77')\n",
        "      plt.plot(plot_ann, label='MLP Prediction', color='#d95f02')\n",
        "      plt.plot(plot_cnn, ':', label='CNN Prediction', color='#7570b3')\n",
        "      plt.plot(plot_lstm, label='LSTM Prediction', color='#e7298a')\n",
        "      plt.plot(plot_gru, label='GRU Prediction', color='#66a61e')\n",
        "      plt.title(\"Prediction\")\n",
        "      plt.xlabel('Time')\n",
        "      plt.ylabel('Normalized Stock Prices')\n",
        "      plt.legend()\n",
        "      plt.savefig('drive/MyDrive/Findan/Sb_1/plots/multi_step/' + sym + ' ' + ' in_sz ' + str(in_size) + ' out_sz ' + str(out_size))\n",
        "      plt.clf()\n",
        "      \n",
        "      print(y_test)\n",
        "      print(plot_ann)\n",
        "      # save prediction plots\n",
        "      df = pd.DataFrame(zip(flat_y_test, plot_ann, plot_cnn, plot_lstm, plot_gru), columns = ['actual', 'ann', 'cnn', 'lstm', 'gru'])\n",
        "      df.to_csv(path_or_buf='drive/MyDrive/Findan/Sb_1/predictions/multi_step/predictions_{}_{}_{}.csv'.format(sym, in_size, out_size))\n",
        "\n",
        "  json_descriptor['ann'] = ann\n",
        "  json_descriptor['lstm'] = lstm\n",
        "  json_descriptor['cnn'] = cnn\n",
        "  json_descriptor['gru'] = gru\n",
        "  result.append(json_descriptor)\n",
        "\n",
        "  with open('data.json', 'w') as f:\n",
        "      json.dump(result, f)"
      ],
      "id": "zof0S6bfyT4e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data loaded for ACC!!\n",
            "X train:  (3704, 30)\n",
            "y train:  (3704, 7)\n",
            "X test:  (67, 30)\n",
            "y test:  (67, 7)\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 2ms/step - loss: 0.0217\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 1ms/step - loss: 0.0037\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 1ms/step - loss: 0.0028\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 1ms/step - loss: 0.0015\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 1ms/step - loss: 0.0014\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 0s 1ms/step - loss: 0.0013\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 0s 1ms/step - loss: 0.0012\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 0s 1ms/step - loss: 0.0011\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 00030: early stopping\n",
            "X train:  (3704, 30, 1)\n",
            "y train:  (3704, 7)\n",
            "X train:  (3704, 30, 1)\n",
            "y train:  (3704, 7)\n",
            "Model: \"sequential_313\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_150 (Conv1D)          (None, 26, 64)            384       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_79 (MaxPooling (None, 13, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten_150 (Flatten)        (None, 832)               0         \n",
            "_________________________________________________________________\n",
            "dense_473 (Dense)            (None, 7)                 5831      \n",
            "=================================================================\n",
            "Total params: 6,215\n",
            "Trainable params: 6,215\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0025\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0056\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0049\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 00004: early stopping\n",
            "Model: \"sequential_314\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_158 (LSTM)              (None, 30, 256)           264192    \n",
            "_________________________________________________________________\n",
            "dropout_308 (Dropout)        (None, 30, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_159 (LSTM)              (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dropout_309 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_474 (Dense)            (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 462,215\n",
            "Trainable params: 462,215\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 24s 181ms/step - loss: 0.0095\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 21s 181ms/step - loss: 0.0095\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 21s 183ms/step - loss: 0.0054\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 21s 183ms/step - loss: 0.0047\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 21s 181ms/step - loss: 0.0053\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 21s 182ms/step - loss: 0.0046\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 21s 181ms/step - loss: 0.0041\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 22s 186ms/step - loss: 0.0042\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 21s 182ms/step - loss: 0.0038\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 21s 183ms/step - loss: 0.0036\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 21s 183ms/step - loss: 0.0032\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 21s 184ms/step - loss: 0.0035\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 21s 185ms/step - loss: 0.0029\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 21s 182ms/step - loss: 0.0029\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 21s 181ms/step - loss: 0.0023\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 21s 184ms/step - loss: 0.0023\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 21s 182ms/step - loss: 0.0026\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 21s 183ms/step - loss: 0.0026\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 21s 182ms/step - loss: 0.0024\n",
            "Epoch 00019: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 19s 138ms/step - loss: 0.0091\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 16s 136ms/step - loss: 0.0074\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 16s 137ms/step - loss: 0.0074\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 16s 138ms/step - loss: 0.0057\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 16s 138ms/step - loss: 0.0046\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 16s 139ms/step - loss: 0.0043\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 16s 137ms/step - loss: 0.0042\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 16s 139ms/step - loss: 0.0047\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 16s 137ms/step - loss: 0.0046\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 16s 138ms/step - loss: 0.0030\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 16s 137ms/step - loss: 0.0030\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 16s 137ms/step - loss: 0.0033\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 16s 137ms/step - loss: 0.0031\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 16s 135ms/step - loss: 0.0024\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 16s 137ms/step - loss: 0.0024\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 16s 137ms/step - loss: 0.0026\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 16s 136ms/step - loss: 0.0023\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 16s 139ms/step - loss: 0.0027\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 16s 140ms/step - loss: 0.0027\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 16s 139ms/step - loss: 0.0023\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 16s 138ms/step - loss: 0.0027\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 16s 138ms/step - loss: 0.0024\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 16s 138ms/step - loss: 0.0023\n",
            "Epoch 00023: early stopping\n",
            "X train:  (3704, 30)\n",
            "y train:  (3704, 7)\n",
            "X test:  (67, 30)\n",
            "y test:  (67, 7)\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 2ms/step - loss: 0.0146\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0036\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 1ms/step - loss: 0.0019\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 1ms/step - loss: 0.0016\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 00026: early stopping\n",
            "X train:  (3704, 30, 1)\n",
            "y train:  (3704, 7)\n",
            "X train:  (3704, 30, 1)\n",
            "y train:  (3704, 7)\n",
            "Model: \"sequential_317\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_151 (Conv1D)          (None, 26, 64)            384       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_80 (MaxPooling (None, 13, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten_151 (Flatten)        (None, 832)               0         \n",
            "_________________________________________________________________\n",
            "dense_479 (Dense)            (None, 7)                 5831      \n",
            "=================================================================\n",
            "Total params: 6,215\n",
            "Trainable params: 6,215\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0026\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0047\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0035\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 31/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 32/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 33/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 00033: early stopping\n",
            "Model: \"sequential_318\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_160 (LSTM)              (None, 30, 256)           264192    \n",
            "_________________________________________________________________\n",
            "dropout_312 (Dropout)        (None, 30, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_161 (LSTM)              (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dropout_313 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_480 (Dense)            (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 462,215\n",
            "Trainable params: 462,215\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 24s 185ms/step - loss: 0.0092\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 22s 188ms/step - loss: 0.0096\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 21s 183ms/step - loss: 0.0079\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 22s 187ms/step - loss: 0.0062\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 22s 186ms/step - loss: 0.0051\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0051\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 23s 194ms/step - loss: 0.0043\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0036\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 22s 192ms/step - loss: 0.0044\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0039\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0032\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 22s 186ms/step - loss: 0.0030\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 22s 188ms/step - loss: 0.0028\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 22s 186ms/step - loss: 0.0029\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 22s 187ms/step - loss: 0.0028\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 22s 186ms/step - loss: 0.0025\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 22s 186ms/step - loss: 0.0025\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 22s 189ms/step - loss: 0.0033\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0025\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 22s 192ms/step - loss: 0.0024\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0027\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 22s 186ms/step - loss: 0.0027\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 22s 189ms/step - loss: 0.0025\n",
            "Epoch 00023: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 20s 141ms/step - loss: 0.0100\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 16s 142ms/step - loss: 0.0107\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 16s 138ms/step - loss: 0.0065\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 16s 141ms/step - loss: 0.0048\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 16s 140ms/step - loss: 0.0047\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 16s 139ms/step - loss: 0.0046\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 16s 140ms/step - loss: 0.0044\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 16s 139ms/step - loss: 0.0040\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 16s 140ms/step - loss: 0.0044\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 16s 141ms/step - loss: 0.0041\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 17s 146ms/step - loss: 0.0030\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 16s 142ms/step - loss: 0.0027\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 16s 140ms/step - loss: 0.0032\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 16s 139ms/step - loss: 0.0025\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 16s 139ms/step - loss: 0.0029\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 16s 139ms/step - loss: 0.0024\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 16s 140ms/step - loss: 0.0025\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 16s 141ms/step - loss: 0.0030\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 16s 140ms/step - loss: 0.0026\n",
            "Epoch 00019: early stopping\n",
            "X train:  (3704, 30)\n",
            "y train:  (3704, 7)\n",
            "X test:  (67, 30)\n",
            "y test:  (67, 7)\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 00015: early stopping\n",
            "X train:  (3704, 30, 1)\n",
            "y train:  (3704, 7)\n",
            "X train:  (3704, 30, 1)\n",
            "y train:  (3704, 7)\n",
            "Model: \"sequential_321\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_152 (Conv1D)          (None, 26, 64)            384       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_81 (MaxPooling (None, 13, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten_152 (Flatten)        (None, 832)               0         \n",
            "_________________________________________________________________\n",
            "dense_485 (Dense)            (None, 7)                 5831      \n",
            "=================================================================\n",
            "Total params: 6,215\n",
            "Trainable params: 6,215\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0025\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0050\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0047\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 00004: early stopping\n",
            "Model: \"sequential_322\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_162 (LSTM)              (None, 30, 256)           264192    \n",
            "_________________________________________________________________\n",
            "dropout_316 (Dropout)        (None, 30, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_163 (LSTM)              (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dropout_317 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_486 (Dense)            (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 462,215\n",
            "Trainable params: 462,215\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 24s 186ms/step - loss: 0.0100\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 22s 189ms/step - loss: 0.0084\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 21s 184ms/step - loss: 0.0077\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 22s 186ms/step - loss: 0.0055\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0044\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 22s 185ms/step - loss: 0.0046\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 22s 185ms/step - loss: 0.0046\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0041\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 21s 184ms/step - loss: 0.0038\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 22s 188ms/step - loss: 0.0032\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0035\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 22s 189ms/step - loss: 0.0036\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0032\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 22s 186ms/step - loss: 0.0029\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 22s 188ms/step - loss: 0.0028\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 23s 200ms/step - loss: 0.0026\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0029\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 22s 188ms/step - loss: 0.0022\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 22s 189ms/step - loss: 0.0022\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 22s 189ms/step - loss: 0.0026\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0024\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 22s 189ms/step - loss: 0.0021\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 22s 189ms/step - loss: 0.0023\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0023\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 22s 192ms/step - loss: 0.0023\n",
            "Epoch 00025: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 20s 143ms/step - loss: 0.0081\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0087\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 17s 145ms/step - loss: 0.0062\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 17s 147ms/step - loss: 0.0048\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 17s 143ms/step - loss: 0.0045\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 17s 143ms/step - loss: 0.0042\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 17s 148ms/step - loss: 0.0040\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0032\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 17s 146ms/step - loss: 0.0039\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 16s 141ms/step - loss: 0.0032\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 17s 143ms/step - loss: 0.0035\n",
            "Epoch 00011: early stopping\n",
            "X train:  (3704, 30)\n",
            "y train:  (3704, 7)\n",
            "X test:  (67, 30)\n",
            "y test:  (67, 7)\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 2ms/step - loss: 0.0372\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0045\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0033\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 00013: early stopping\n",
            "X train:  (3704, 30, 1)\n",
            "y train:  (3704, 7)\n",
            "X train:  (3704, 30, 1)\n",
            "y train:  (3704, 7)\n",
            "Model: \"sequential_325\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_153 (Conv1D)          (None, 26, 64)            384       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_82 (MaxPooling (None, 13, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten_153 (Flatten)        (None, 832)               0         \n",
            "_________________________________________________________________\n",
            "dense_491 (Dense)            (None, 7)                 5831      \n",
            "=================================================================\n",
            "Total params: 6,215\n",
            "Trainable params: 6,215\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0025\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0051\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0049\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 00004: early stopping\n",
            "Model: \"sequential_326\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_164 (LSTM)              (None, 30, 256)           264192    \n",
            "_________________________________________________________________\n",
            "dropout_320 (Dropout)        (None, 30, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_165 (LSTM)              (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dropout_321 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_492 (Dense)            (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 462,215\n",
            "Trainable params: 462,215\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 25s 193ms/step - loss: 0.0094\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0111\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0068\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 22s 187ms/step - loss: 0.0052\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 22s 186ms/step - loss: 0.0044\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0044\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 22s 189ms/step - loss: 0.0053\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0047\n",
            "Epoch 00008: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 20s 143ms/step - loss: 0.0090\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0074\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 17s 143ms/step - loss: 0.0054\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 17s 146ms/step - loss: 0.0052\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 17s 142ms/step - loss: 0.0050\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0047\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 17s 145ms/step - loss: 0.0041\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 17s 143ms/step - loss: 0.0042\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0041\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 16s 140ms/step - loss: 0.0032\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0032\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 16s 140ms/step - loss: 0.0030\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 17s 143ms/step - loss: 0.0027\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 16s 141ms/step - loss: 0.0030\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 17s 142ms/step - loss: 0.0026\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 17s 143ms/step - loss: 0.0031\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 16s 141ms/step - loss: 0.0025\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0028\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 16s 140ms/step - loss: 0.0031\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 16s 139ms/step - loss: 0.0026\n",
            "Epoch 00020: early stopping\n",
            "X train:  (3704, 30)\n",
            "y train:  (3704, 7)\n",
            "X test:  (67, 30)\n",
            "y test:  (67, 7)\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 2ms/step - loss: 0.0076\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0033\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 00019: early stopping\n",
            "X train:  (3704, 30, 1)\n",
            "y train:  (3704, 7)\n",
            "X train:  (3704, 30, 1)\n",
            "y train:  (3704, 7)\n",
            "Model: \"sequential_329\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_154 (Conv1D)          (None, 26, 64)            384       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_83 (MaxPooling (None, 13, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten_154 (Flatten)        (None, 832)               0         \n",
            "_________________________________________________________________\n",
            "dense_497 (Dense)            (None, 7)                 5831      \n",
            "=================================================================\n",
            "Total params: 6,215\n",
            "Trainable params: 6,215\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0025\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0049\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0058\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 00004: early stopping\n",
            "Model: \"sequential_330\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_166 (LSTM)              (None, 30, 256)           264192    \n",
            "_________________________________________________________________\n",
            "dropout_324 (Dropout)        (None, 30, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_167 (LSTM)              (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dropout_325 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_498 (Dense)            (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 462,215\n",
            "Trainable params: 462,215\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 25s 192ms/step - loss: 0.0112\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 22s 188ms/step - loss: 0.0082\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 22s 192ms/step - loss: 0.0066\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0058\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 22s 189ms/step - loss: 0.0054\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0046\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0045\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0037\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0041\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0034\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0034\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0033\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0028\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0031\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0029\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0027\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0030\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0027\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0024\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0025\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0025\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0024\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0025\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0023\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 22s 189ms/step - loss: 0.0024\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 23s 194ms/step - loss: 0.0022\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 22s 192ms/step - loss: 0.0021\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0021\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 22s 189ms/step - loss: 0.0025\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0024\n",
            "Epoch 31/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0021\n",
            "Epoch 32/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0019\n",
            "Epoch 33/200\n",
            "116/116 [==============================] - 22s 192ms/step - loss: 0.0019\n",
            "Epoch 34/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0022\n",
            "Epoch 35/200\n",
            "116/116 [==============================] - 22s 189ms/step - loss: 0.0021\n",
            "Epoch 36/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0018\n",
            "Epoch 37/200\n",
            "116/116 [==============================] - 22s 192ms/step - loss: 0.0023\n",
            "Epoch 38/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0023\n",
            "Epoch 39/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0020\n",
            "Epoch 00039: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 20s 144ms/step - loss: 0.0095\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 17s 143ms/step - loss: 0.0114\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 16s 142ms/step - loss: 0.0060\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 17s 146ms/step - loss: 0.0054\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0047\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0047\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 17s 147ms/step - loss: 0.0046\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0039\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0038\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0033\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 17s 145ms/step - loss: 0.0030\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 17s 145ms/step - loss: 0.0032\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 17s 145ms/step - loss: 0.0036\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 17s 146ms/step - loss: 0.0035\n",
            "Epoch 00014: early stopping\n",
            "[[0.81817348 0.80431857 0.81224003 0.81826523 0.81517617 0.8193357\n",
            "  0.80719354]\n",
            " [0.80138243 0.7887815  0.78511133 0.7835515  0.78287864 0.77422315\n",
            "  0.77407022]\n",
            " [0.77033888 0.77682285 0.77926964 0.77807683 0.78749694 0.79055542\n",
            "  0.79153413]\n",
            " [0.78511133 0.78339858 0.78605946 0.76975777 0.7785356  0.77590531\n",
            "  0.7761194 ]\n",
            " [0.78859799 0.79235992 0.78966846 0.80624541 0.81597137 0.82101786\n",
            "  0.83297651]\n",
            " [0.8197333  0.82321997 0.83306827 0.83591265 0.83566797 0.84120382\n",
            "  0.83487277]\n",
            " [0.84618914 0.84600563 0.83646318 0.90469782 0.90488133 0.90139467\n",
            "  0.922437  ]\n",
            " [0.91665647 0.89616467 0.90564595 0.91512723 0.93720944 0.96846709\n",
            "  0.96231955]\n",
            " [0.95568265 0.95504037 0.94925985 0.95051382 0.97972229 0.95794593\n",
            "  0.94347932]\n",
            " [0.9465378  0.91683998 0.91723758 0.88998654 0.91246636 0.92020431\n",
            "  0.91534133]\n",
            " [0.92191705 0.92307928 0.93595547 0.92671886 0.92837044 0.90469782\n",
            "  0.90784805]\n",
            " [0.91206875 0.91188525 0.90561537 0.91008074 0.89913139 0.90781747\n",
            "  0.9240274 ]\n",
            " [0.92586249 0.93262173 0.93571079 0.93161243 0.92286518 0.88729508\n",
            "  0.88821263]\n",
            " [0.88815146 0.88087228 0.89283093 0.88857964 0.89702104 0.90754221\n",
            "  0.90613531]\n",
            " [0.9176046  0.93149009 0.9652251  0.96375703 0.99528994 0.99057989\n",
            "  0.9979814 ]\n",
            " [0.98275018 0.96892586 0.9668461  0.97678615 0.97755077 0.96546978\n",
            "  0.98115977]\n",
            " [0.98067042 0.98189381 0.98467702 1.00568877 1.02923905 1.03092121\n",
            "  1.033368  ]\n",
            " [1.03018718 1.03468314 1.00850257 0.99862368 1.0134573  1.02960607\n",
            "  1.01058233]\n",
            " [1.01926841 1.01822853 1.01620993 1.0239173  1.01143871 1.0323587\n",
            "  1.02367262]\n",
            " [1.02960607 1.022908   1.03379618 1.00507707 1.02578297 1.02709812\n",
            "  1.01465011]\n",
            " [1.01740274 1.02125642 1.05410448 1.04951676 1.05018963 1.03541718\n",
            "  1.03856741]\n",
            " [1.03327624 1.01422192 0.99204796 0.95442868 0.92038782 0.92005138\n",
            "  0.89414607]\n",
            " [0.9284622  0.93522143 0.9292574  0.93164301 0.96167727 0.98580866\n",
            "  0.97880475]\n",
            " [0.98241375 0.97103621 0.98669562 1.00443479 1.01645461 1.01816736\n",
            "  1.01281502]\n",
            " [1.00143748 1.01113286 1.00911426 1.02348911 1.01201982 1.01244801\n",
            "  1.03471373]\n",
            " [1.02856619 1.03703817 1.02238806 1.02593589 1.02501835 1.01663812\n",
            "  1.01529239]\n",
            " [0.99865427 0.99480059 0.99629924 0.98791901 1.00492415 1.0200942\n",
            "  1.00015292]\n",
            " [0.96589797 0.9648275  0.97519574 0.96907879 0.96017862 0.95748715\n",
            "  0.95011622]\n",
            " [0.95088084 0.94195009 0.94730242 0.95794593 0.96048446 0.95552973\n",
            "  0.96369586]\n",
            " [0.99030462 0.98164913 0.9678554  0.96525569 0.95436751 0.95944458\n",
            "  0.95965867]\n",
            " [0.98011989 0.97822364 0.98534989 0.97482873 0.9873379  0.98348422\n",
            "  0.99134451]\n",
            " [0.99730854 0.98076217 0.98801077 1.00721801 1.02358087 1.02884145\n",
            "  1.03804747]\n",
            " [1.03012601 1.03287864 1.02403964 1.02578297 1.04110595 1.05349278\n",
            "  1.04752875]\n",
            " [1.04661121 1.0408001  1.0393932  1.01672988 0.9845241  0.97773428\n",
            "  0.98525814]\n",
            " [0.97877416 0.97091387 0.98421825 0.94922926 0.92558723 0.91200759\n",
            "  0.89270859]\n",
            " [0.95745657 0.9376988  0.93194886 0.91858331 0.91344507 0.93136775\n",
            "  0.9210301 ]\n",
            " [0.92509787 0.92809518 0.92020431 0.93188769 0.93775997 0.92047957\n",
            "  0.91451554]\n",
            " [0.92109126 0.90417788 0.89754098 0.8852459  0.86674211 0.8587289\n",
            "  0.87588696]\n",
            " [0.88533766 0.9077563  0.90234279 0.88081111 0.87772205 0.87276731\n",
            "  0.87496942]\n",
            " [0.86946415 0.8567103  0.8595241  0.85392709 0.84383411 0.86184854\n",
            "  0.87276731]\n",
            " [0.85759726 0.87126866 0.87181918 0.86450942 0.86319427 0.85976878\n",
            "  0.8545082 ]\n",
            " [0.8615427  0.88604111 0.89243333 0.88420602 0.88289087 0.87695743\n",
            "  0.87919011]\n",
            " [0.87717152 0.8806276  0.86701737 0.87570345 0.89255566 0.8742048\n",
            "  0.86236849]\n",
            " [0.85622094 0.85362124 0.85065451 0.84514925 0.83548446 0.81878517\n",
            "  0.80939564]\n",
            " [0.78520308 0.78468314 0.77838268 0.76238684 0.74388304 0.73238317\n",
            "  0.72531808]\n",
            " [0.73697088 0.73088451 0.73067042 0.72302422 0.72265721 0.75030585\n",
            "  0.73323954]\n",
            " [0.72183142 0.70776242 0.72666381 0.72696966 0.72614387 0.72926352\n",
            "  0.72651089]\n",
            " [0.73583925 0.72131148 0.71819183 0.71715195 0.70745657 0.70173722\n",
            "  0.69858698]\n",
            " [0.7136347  0.72641913 0.75214093 0.72229019 0.7116161  0.74131392\n",
            "  0.72923293]\n",
            " [0.73700147 0.74801199 0.77177636 0.76198923 0.76327379 0.76033766\n",
            "  0.75394544]\n",
            " [0.74749205 0.73993761 0.69662956 0.70978101 0.71183019 0.70130903\n",
            "  0.7126254 ]\n",
            " [0.72351358 0.82425985 0.82068143 0.85313188 0.86065574 0.84157083\n",
            "  0.85744434]\n",
            " [0.85842305 0.86438708 0.85065451 0.85392709 0.86288843 0.86145094\n",
            "  0.88252386]\n",
            " [0.8669562  0.85316247 0.87811965 0.87536702 0.89671519 0.89998777\n",
            "  0.92476144]\n",
            " [0.92197822 0.92540372 0.93228529 0.91488255 0.90867384 0.91512723\n",
            "  0.92411916]\n",
            " [0.93840225 0.8932897  0.86873012 0.87448006 0.88325789 0.85667972\n",
            "  0.85111329]\n",
            " [0.87252263 0.90179227 0.89231099 0.88221801 0.8782114  0.86346954\n",
            "  0.85603744]\n",
            " [0.87827257 0.88949719 0.87671275 0.87126866 0.88426719 0.84517984\n",
            "  0.8563127 ]\n",
            " [0.8261561  0.82373991 0.81835699 0.81701126 0.79936384 0.84120382\n",
            "  0.85340714]\n",
            " [0.87191094 0.86353071 0.7930022  0.79162589 0.76645461 0.76724982\n",
            "  0.75825789]\n",
            " [0.74727795 0.74773673 0.7443724  0.7626621  0.78156349 0.80957915\n",
            "  0.8235564 ]\n",
            " [0.80710179 0.81618547 0.81725593 0.80174945 0.81306582 0.82328114\n",
            "  0.84753487]\n",
            " [0.84010276 0.84025569 0.82086494 0.83747247 0.81263763 0.80199413\n",
            "  0.80569489]\n",
            " [0.80067898 0.81710301 0.8303768  0.84952288 0.84472107 0.82159897\n",
            "  0.79263518]\n",
            " [0.80441033 0.78529484 0.79817103 0.8301933  0.83976633 0.83780891\n",
            "  0.84046978]\n",
            " [0.84001101 0.86609983 0.86203205 0.83316002 0.80722413 0.82664546\n",
            "  0.81428921]\n",
            " [0.82682897 0.84420113 0.8323954  0.82942868 0.81003793 0.820345\n",
            "  0.8257585 ]]\n",
            "[0.8480683326721191, 0.850538682937622, 0.8492719888687134, 0.8581799387931823, 0.8610064387321472, 0.854245948791504, 0.8510567784309387, 0.8439387321472168, 0.8389225244522095, 0.8438974499702454, 0.8514918088912964, 0.8531902432441711, 0.8424894094467164, 0.8500806331634522, 0.815438175201416, 0.8102973699569702, 0.8177322626113892, 0.8153187990188598, 0.8284974932670593, 0.8137802481651306, 0.823286521434784, 0.8099705219268799, 0.8074485063552856, 0.8060568332672119, 0.8062362551689148, 0.8217658877372742, 0.8050075054168702, 0.8145372986793518, 0.8071045041084289, 0.8091336369514466, 0.8021953701972961, 0.8102127909660339, 0.8170239329338074, 0.8131433248519897, 0.8142378807067872, 0.8313841342926025, 0.8280822396278381, 0.8264254808425904, 0.8326793551445008, 0.8401307940483094, 0.829665732383728, 0.833782148361206, 0.854940676689148, 0.8550206542015075, 0.8559586524963378, 0.8602606773376464, 0.8686485648155212, 0.8547907829284668, 0.8600520014762878, 0.8979937791824341, 0.8951850652694702, 0.8973924517631531, 0.9048300862312317, 0.9103084325790405, 0.8974030137062072, 0.899963092803955, 0.9356517553329468, 0.9357904195785522, 0.9380984902381897, 0.9478422999382019, 0.9536690592765809, 0.9377921342849731, 0.9413087010383606, 0.972692346572876, 0.9678925752639771, 0.974916398525238, 0.978377890586853, 0.9814693927764893, 0.9696002960205078, 0.9706831574440002, 0.9403499484062194, 0.9341784358024597, 0.9411306023597718, 0.9435536861419678, 0.9518672823905945, 0.9401531338691711, 0.9456471681594849, 0.9303518295288086, 0.9373918414115906, 0.937318742275238, 0.9327990770339966, 0.9493474006652832, 0.9318832874298095, 0.9429216861724854, 0.923835563659668, 0.9288470983505249, 0.9194793224334716, 0.9288116097450256, 0.9394413113594056, 0.9329362750053406, 0.9364142894744873, 0.9286739587783813, 0.9350815892219544, 0.9286591529846191, 0.9343695640563965, 0.9436042070388794, 0.9338035345077514, 0.941959547996521, 0.9118296146392822, 0.9117326617240906, 0.9080479264259338, 0.9154218912124634, 0.9283009529113769, 0.9171961307525635, 0.926036536693573, 0.965799081325531, 0.9731272220611572, 0.9723302841186523, 0.971862506866455, 0.9891140341758728, 0.9631467342376709, 0.9798739910125732, 0.9911198258399964, 0.9893062829971313, 0.9885947704315186, 0.9997070789337158, 1.0019267678260804, 0.9941472053527832, 0.9973628282546997, 1.0115335702896118, 1.0078731060028077, 1.0157236576080322, 1.0179832100868225, 1.0277255296707153, 1.0056198835372925, 1.0172402620315553, 1.0191306829452516, 1.0247001647949219, 1.027899146080017, 1.0276305198669433, 1.043410873413086, 1.025410771369934, 1.0331424713134765, 1.027940535545349, 1.0278267860412598, 1.0279545068740845, 1.0320480585098266, 1.0406572818756104, 1.0281558156013488, 1.0352622270584106, 1.023740530014038, 1.0332845449447632, 1.0305268526077271, 1.031333899497986, 1.0462367534637451, 1.0285655021667481, 1.0396087169647217, 1.0431851625442505, 1.0468777179718018, 1.0412535429000855, 1.0452348947525025, 1.0577361106872558, 1.0428089857101441, 1.0542725563049316, 0.9800734877586365, 0.98141850233078, 0.9789896965026855, 0.9844955921173095, 0.9902303695678711, 0.9901066541671752, 0.9935317277908325, 0.9538894534111023, 0.9574699163436889, 0.9530006289482117, 0.9521931767463684, 0.9741084337234497, 0.9549545288085938, 0.9694504380226135, 0.9942737698554993, 1.0104655861854552, 0.9987339615821839, 0.998311448097229, 1.023731279373169, 1.0015501737594605, 1.0178605556488036, 1.0286977291107178, 1.0234752893447876, 1.0179063558578492, 1.0351895809173584, 1.030504536628723, 1.0326195001602172, 1.0311357736587525, 1.0275225162506103, 1.031528115272522, 1.035082793235779, 1.0346456527709962, 1.0490751266479492, 1.0290618896484376, 1.0375661373138427, 1.007787036895752, 1.0104753971099854, 1.00637788772583, 1.012590193748474, 1.0263981342315673, 1.0130940914154052, 1.0231865406036378, 0.9766514778137207, 0.9797123908996582, 0.9766197681427002, 0.978597891330719, 0.9902375698089599, 0.979330313205719, 0.9925273299217224, 0.9655453324317932, 0.969116497039795, 0.9597420454025268, 0.966270923614502, 0.9791133999824524, 0.96617351770401, 0.9764567971229553, 0.9739437341690064, 0.9783492088317871, 0.9682816743850708, 0.9753499984741211, 0.9877874374389648, 0.9778064370155335, 0.984214448928833, 0.9905385375022888, 0.9912012934684753, 0.9852263808250428, 0.9927379012107849, 1.0021804094314575, 0.9915309429168702, 0.9973234176635742, 1.0107018113136292, 1.0163597822189332, 1.0157804489135742, 1.0178238153457642, 1.030656099319458, 1.0144670724868774, 1.0248444557189942, 1.0405806303024292, 1.0410246133804322, 1.043458914756775, 1.0477732419967651, 1.0553695678710937, 1.0414799451828003, 1.04901180267334, 1.0239764213562013, 1.024240207672119, 1.0278542041778564, 1.029250478744507, 1.039323353767395, 1.0283552289009095, 1.034852433204651, 0.9589451909065246, 0.9645064830780029, 0.9624834299087525, 0.9622240662574768, 0.9756602764129638, 0.9649070382118226, 0.9741567850112915, 0.9347684860229493, 0.9426629304885864, 0.9352185487747192, 0.9336938261985779, 0.9533241987228394, 0.9409479141235352, 0.9537450551986695, 0.9380530834197998, 0.9441950917243958, 0.9312535405158997, 0.9368690609931946, 0.9529713749885559, 0.9426050782203674, 0.9539524674415588, 0.9072453141212463, 0.907798683643341, 0.903289520740509, 0.9111424803733825, 0.9167253375053406, 0.9171793103218079, 0.9229992508888245, 0.897627079486847, 0.903237271308899, 0.9015261650085449, 0.8978813767433167, 0.9190730094909668, 0.9013165354728698, 0.9119397163391113, 0.8803462862968445, 0.8839967370033264, 0.8757524609565734, 0.8825209140777588, 0.8952448964118958, 0.8879376173019409, 0.894842004776001, 0.8821961045265198, 0.8834858536720276, 0.8808918476104737, 0.8841630339622497, 0.8921479463577271, 0.8819234251976014, 0.8924110770225525, 0.8999855756759644, 0.9012735247611999, 0.8976959824562073, 0.9016206741333008, 0.9147605538368225, 0.8996430277824402, 0.9076033711433411, 0.8960775375366211, 0.8987436175346375, 0.8940637111663818, 0.901721203327179, 0.9082812666893005, 0.9003024458885193, 0.9050020337104797, 0.8655640006065368, 0.8614805936813354, 0.8631169438362122, 0.8672720432281494, 0.8737727284431458, 0.8649129986763, 0.8761376023292542, 0.7923196792602539, 0.7915350914001464, 0.7919237017631531, 0.7917740106582641, 0.8039986848831177, 0.7969159483909607, 0.8048314213752746, 0.7585591435432434, 0.7606862664222718, 0.7534333229064941, 0.7556298494338989, 0.7720638036727905, 0.7608120560646057, 0.7714516639709472, 0.758358108997345, 0.7580260992050171, 0.7456000566482544, 0.7553372979164124, 0.7666499972343445, 0.7606536507606506, 0.7662281274795533, 0.7542139768600464, 0.7475263118743897, 0.743454885482788, 0.752536940574646, 0.7530483722686767, 0.7540015339851379, 0.7529562473297119, 0.7597040653228759, 0.7574909687042236, 0.7573439121246338, 0.7596695303916932, 0.772217082977295, 0.7609141230583191, 0.7625783920288086, 0.7839914679527282, 0.7898658514022827, 0.7870563507080078, 0.7911776423454284, 0.7991540551185607, 0.7908477544784546, 0.7906830191612244, 0.7603384494781494, 0.7523511528968811, 0.7556865811347961, 0.7658920764923096, 0.7608572721481324, 0.7654007792472839, 0.7629059672355651, 0.8330840587615966, 0.8355538487434387, 0.8412968277931213, 0.8365283012390137, 0.8602987408638001, 0.8221303343772888, 0.8426010251045227, 0.8734997749328614, 0.8827306509017945, 0.8711414575576782, 0.8905982613563538, 0.8932598829269409, 0.8897738933563233, 0.88341783285141, 0.9088074922561645, 0.8915145754814148, 0.9059438228607177, 0.912453043460846, 0.9101584672927856, 0.8919782519340516, 0.9087852835655212, 0.9283751964569091, 0.9286310911178589, 0.9411866784095764, 0.9340290665626526, 0.9522815465927124, 0.9179457426071167, 0.935364055633545, 0.9117223620414734, 0.9040021896362305, 0.9072774052619934, 0.9128182411193848, 0.9154255747795105, 0.9087425351142884, 0.9094557642936707, 0.8894669532775878, 0.8910366773605347, 0.8952359080314636, 0.8912055492401123, 0.9066247463226318, 0.889570152759552, 0.9036363244056702, 0.8855886936187745, 0.898770534992218, 0.888173234462738, 0.8918946504592895, 0.90755136013031, 0.8983125567436219, 0.9027499437332154, 0.8485068321228028, 0.8475811839103699, 0.8438967585563659, 0.8554167866706848, 0.8572350859642028, 0.8591592073440552, 0.8648492932319641, 0.8270835995674133, 0.8269593715667725, 0.8291425943374634, 0.8237969040870666, 0.8375134110450745, 0.8252414584159851, 0.8349863290786743, 0.8058722853660584, 0.7985683083534241, 0.7917225003242493, 0.7981480479240417, 0.8157937049865722, 0.8014270901679993, 0.8105971813201904, 0.8332737684249878, 0.8423141956329345, 0.8348867535591126, 0.836742901802063, 0.8459230542182923, 0.8394757747650147, 0.8451227426528931, 0.8632870554924011, 0.8509251832962036, 0.8466007471084595, 0.8599017858505249, 0.8544145107269288, 0.8557517886161804, 0.8491165637969971, 0.8483101010322571, 0.8469385385513306, 0.847972321510315, 0.8515880823135376, 0.8614970803260803, 0.8450265407562256, 0.8526084780693054, 0.840705955028534, 0.843068265914917, 0.8396783947944642, 0.8482043981552124, 0.8573617696762085, 0.8488678455352783, 0.8550611853599548, 0.8555418610572815, 0.8561977505683899, 0.8565398097038269, 0.8605575680732727, 0.8669672846794129, 0.8565835595130921, 0.8649392366409302]\n",
            "X train:  (3697, 30)\n",
            "y train:  (3697, 14)\n",
            "X test:  (33, 30)\n",
            "y test:  (33, 14)\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 2ms/step - loss: 0.0115\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0046\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0028\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 00012: early stopping\n",
            "X train:  (3697, 30, 1)\n",
            "y train:  (3697, 14)\n",
            "X train:  (3697, 30, 1)\n",
            "y train:  (3697, 14)\n",
            "Model: \"sequential_333\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_155 (Conv1D)          (None, 26, 64)            384       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_84 (MaxPooling (None, 13, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten_155 (Flatten)        (None, 832)               0         \n",
            "_________________________________________________________________\n",
            "dense_503 (Dense)            (None, 14)                11662     \n",
            "=================================================================\n",
            "Total params: 12,046\n",
            "Trainable params: 12,046\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0064\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0051\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 00004: early stopping\n",
            "Model: \"sequential_334\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_168 (LSTM)              (None, 30, 256)           264192    \n",
            "_________________________________________________________________\n",
            "dropout_328 (Dropout)        (None, 30, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_169 (LSTM)              (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dropout_329 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_504 (Dense)            (None, 14)                1806      \n",
            "=================================================================\n",
            "Total params: 463,118\n",
            "Trainable params: 463,118\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 26s 192ms/step - loss: 0.0126\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0088\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0083\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0079\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0070\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 22s 192ms/step - loss: 0.0058\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0058\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0043\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 22s 191ms/step - loss: 0.0046\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 22s 192ms/step - loss: 0.0041\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0037\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 22s 192ms/step - loss: 0.0039\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0033\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 22s 189ms/step - loss: 0.0030\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0032\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0031\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 22s 190ms/step - loss: 0.0035\n",
            "Epoch 00017: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 20s 148ms/step - loss: 0.0113\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 17s 147ms/step - loss: 0.0201\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 17s 147ms/step - loss: 0.0088\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 17s 150ms/step - loss: 0.0068\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 17s 147ms/step - loss: 0.0067\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 17s 146ms/step - loss: 0.0058\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 17s 147ms/step - loss: 0.0050\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 17s 145ms/step - loss: 0.0049\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 17s 145ms/step - loss: 0.0047\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 17s 145ms/step - loss: 0.0043\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 17s 148ms/step - loss: 0.0037\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 17s 145ms/step - loss: 0.0035\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 17s 147ms/step - loss: 0.0034\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 17s 146ms/step - loss: 0.0035\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 17s 145ms/step - loss: 0.0032\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 17s 145ms/step - loss: 0.0036\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0034\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0036\n",
            "Epoch 00018: early stopping\n",
            "X train:  (3697, 30)\n",
            "y train:  (3697, 14)\n",
            "X test:  (33, 30)\n",
            "y test:  (33, 14)\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 16s 133ms/step - loss: 0.0175\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0037\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0028\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 00023: early stopping\n",
            "X train:  (3697, 30, 1)\n",
            "y train:  (3697, 14)\n",
            "X train:  (3697, 30, 1)\n",
            "y train:  (3697, 14)\n",
            "Model: \"sequential_337\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_156 (Conv1D)          (None, 26, 64)            384       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_85 (MaxPooling (None, 13, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten_156 (Flatten)        (None, 832)               0         \n",
            "_________________________________________________________________\n",
            "dense_509 (Dense)            (None, 14)                11662     \n",
            "=================================================================\n",
            "Total params: 12,046\n",
            "Trainable params: 12,046\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0066\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0049\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 00004: early stopping\n",
            "Model: \"sequential_338\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_170 (LSTM)              (None, 30, 256)           264192    \n",
            "_________________________________________________________________\n",
            "dropout_332 (Dropout)        (None, 30, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_171 (LSTM)              (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dropout_333 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_510 (Dense)            (None, 14)                1806      \n",
            "=================================================================\n",
            "Total params: 463,118\n",
            "Trainable params: 463,118\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 26s 197ms/step - loss: 0.0137\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0092\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 23s 198ms/step - loss: 0.0077\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0064\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 23s 199ms/step - loss: 0.0055\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0056\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 23s 198ms/step - loss: 0.0058\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0053\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 23s 198ms/step - loss: 0.0047\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 23s 194ms/step - loss: 0.0042\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0038\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0038\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0037\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0034\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0033\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 23s 194ms/step - loss: 0.0034\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0028\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 23s 194ms/step - loss: 0.0031\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0030\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0032\n",
            "Epoch 00020: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 20s 148ms/step - loss: 0.0107\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 17s 147ms/step - loss: 0.0155\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0079\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0079\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 17s 146ms/step - loss: 0.0056\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 17s 144ms/step - loss: 0.0045\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 17s 147ms/step - loss: 0.0046\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0040\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 17s 148ms/step - loss: 0.0037\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 17s 146ms/step - loss: 0.0039\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 17s 146ms/step - loss: 0.0039\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 17s 146ms/step - loss: 0.0040\n",
            "Epoch 00012: early stopping\n",
            "X train:  (3697, 30)\n",
            "y train:  (3697, 14)\n",
            "X test:  (33, 30)\n",
            "y test:  (33, 14)\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 2ms/step - loss: 0.1781\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0743\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0157\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0083\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0067\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0049\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0033\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 31/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 32/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 33/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 34/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 35/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 36/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 37/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 38/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 39/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 00039: early stopping\n",
            "X train:  (3697, 30, 1)\n",
            "y train:  (3697, 14)\n",
            "X train:  (3697, 30, 1)\n",
            "y train:  (3697, 14)\n",
            "Model: \"sequential_341\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_157 (Conv1D)          (None, 26, 64)            384       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_86 (MaxPooling (None, 13, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten_157 (Flatten)        (None, 832)               0         \n",
            "_________________________________________________________________\n",
            "dense_515 (Dense)            (None, 14)                11662     \n",
            "=================================================================\n",
            "Total params: 12,046\n",
            "Trainable params: 12,046\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0033\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0057\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 1s 7ms/step - loss: 0.0023\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 1s 7ms/step - loss: 0.0022\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0022\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0021\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0021\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0021\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0020\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0019\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0019\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0019\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0019\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.0019\n",
            "Epoch 00030: early stopping\n",
            "Model: \"sequential_342\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_172 (LSTM)              (None, 30, 256)           264192    \n",
            "_________________________________________________________________\n",
            "dropout_336 (Dropout)        (None, 30, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_173 (LSTM)              (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dropout_337 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_516 (Dense)            (None, 14)                1806      \n",
            "=================================================================\n",
            "Total params: 463,118\n",
            "Trainable params: 463,118\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 26s 196ms/step - loss: 0.0118\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 23s 194ms/step - loss: 0.0097\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 23s 194ms/step - loss: 0.0072\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 23s 198ms/step - loss: 0.0068\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 23s 198ms/step - loss: 0.0058\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 23s 198ms/step - loss: 0.0055\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0048\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 23s 199ms/step - loss: 0.0046\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0045\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0040\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 23s 198ms/step - loss: 0.0039\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0035\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 23s 198ms/step - loss: 0.0035\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0033\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0031\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0034\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0033\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0029\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0030\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0028\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0026\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0027\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0026\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0027\n",
            "Epoch 00024: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 21s 148ms/step - loss: 0.0117\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 17s 148ms/step - loss: 0.0155\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 17s 150ms/step - loss: 0.0098\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 17s 147ms/step - loss: 0.0083\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 17s 148ms/step - loss: 0.0066\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 17s 148ms/step - loss: 0.0053\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 17s 146ms/step - loss: 0.0051\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 17s 150ms/step - loss: 0.0042\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 17s 148ms/step - loss: 0.0041\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 17s 148ms/step - loss: 0.0048\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 17s 146ms/step - loss: 0.0046\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0035\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 17s 147ms/step - loss: 0.0036\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 17s 147ms/step - loss: 0.0033\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 17s 147ms/step - loss: 0.0033\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0030\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 17s 145ms/step - loss: 0.0033\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0037\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 17s 148ms/step - loss: 0.0034\n",
            "Epoch 00019: early stopping\n",
            "X train:  (3697, 30)\n",
            "y train:  (3697, 14)\n",
            "X test:  (33, 30)\n",
            "y test:  (33, 14)\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 2ms/step - loss: 0.0069\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0035\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 00019: early stopping\n",
            "X train:  (3697, 30, 1)\n",
            "y train:  (3697, 14)\n",
            "X train:  (3697, 30, 1)\n",
            "y train:  (3697, 14)\n",
            "Model: \"sequential_345\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_158 (Conv1D)          (None, 26, 64)            384       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_87 (MaxPooling (None, 13, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten_158 (Flatten)        (None, 832)               0         \n",
            "_________________________________________________________________\n",
            "dense_521 (Dense)            (None, 14)                11662     \n",
            "=================================================================\n",
            "Total params: 12,046\n",
            "Trainable params: 12,046\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.0032\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0067\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0046\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 00004: early stopping\n",
            "Model: \"sequential_346\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_174 (LSTM)              (None, 30, 256)           264192    \n",
            "_________________________________________________________________\n",
            "dropout_340 (Dropout)        (None, 30, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_175 (LSTM)              (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dropout_341 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_522 (Dense)            (None, 14)                1806      \n",
            "=================================================================\n",
            "Total params: 463,118\n",
            "Trainable params: 463,118\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 25s 194ms/step - loss: 0.0117\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0105\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0068\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0065\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 23s 194ms/step - loss: 0.0065\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 22s 194ms/step - loss: 0.0056\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0051\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0052\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 23s 194ms/step - loss: 0.0042\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0039\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0039\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0035\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 23s 194ms/step - loss: 0.0033\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0035\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0036\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0035\n",
            "Epoch 00016: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 20s 151ms/step - loss: 0.0120\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0170\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0082\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 17s 150ms/step - loss: 0.0081\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 17s 150ms/step - loss: 0.0062\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 17s 147ms/step - loss: 0.0057\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 17s 148ms/step - loss: 0.0065\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 18s 153ms/step - loss: 0.0046\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 18s 152ms/step - loss: 0.0035\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 17s 151ms/step - loss: 0.0039\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0035\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 17s 151ms/step - loss: 0.0037\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0038\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0037\n",
            "Epoch 00014: early stopping\n",
            "X train:  (3697, 30)\n",
            "y train:  (3697, 14)\n",
            "X test:  (33, 30)\n",
            "y test:  (33, 14)\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 2ms/step - loss: 0.0440\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0037\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 00015: early stopping\n",
            "X train:  (3697, 30, 1)\n",
            "y train:  (3697, 14)\n",
            "X train:  (3697, 30, 1)\n",
            "y train:  (3697, 14)\n",
            "Model: \"sequential_349\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_159 (Conv1D)          (None, 26, 64)            384       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_88 (MaxPooling (None, 13, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten_159 (Flatten)        (None, 832)               0         \n",
            "_________________________________________________________________\n",
            "dense_527 (Dense)            (None, 14)                11662     \n",
            "=================================================================\n",
            "Total params: 12,046\n",
            "Trainable params: 12,046\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 1s 9ms/step - loss: 0.0035\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0069\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0045\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0020\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0020\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0020\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 31/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 32/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 33/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 34/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 35/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 36/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 37/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 38/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 39/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 40/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 41/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 42/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 43/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 44/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 45/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 46/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 47/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 48/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 49/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 50/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 51/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 52/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 53/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 54/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 55/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 56/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 57/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 58/200\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 59/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 60/200\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 00060: early stopping\n",
            "Model: \"sequential_350\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_176 (LSTM)              (None, 30, 256)           264192    \n",
            "_________________________________________________________________\n",
            "dropout_344 (Dropout)        (None, 30, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_177 (LSTM)              (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dropout_345 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_528 (Dense)            (None, 14)                1806      \n",
            "=================================================================\n",
            "Total params: 463,118\n",
            "Trainable params: 463,118\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 26s 196ms/step - loss: 0.0134\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0086\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 23s 198ms/step - loss: 0.0073\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0066\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0059\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0060\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 23s 201ms/step - loss: 0.0052\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0046\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0042\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0043\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0042\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 23s 198ms/step - loss: 0.0038\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0034\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0033\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 22s 194ms/step - loss: 0.0034\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0031\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0030\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0034\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0029\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 23s 198ms/step - loss: 0.0029\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0028\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 23s 198ms/step - loss: 0.0028\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0026\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0025\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0030\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 22s 192ms/step - loss: 0.0027\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 22s 193ms/step - loss: 0.0025\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0027\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 23s 195ms/step - loss: 0.0025\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 23s 197ms/step - loss: 0.0023\n",
            "Epoch 31/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0027\n",
            "Epoch 32/200\n",
            "116/116 [==============================] - 23s 196ms/step - loss: 0.0025\n",
            "Epoch 33/200\n",
            "116/116 [==============================] - 23s 199ms/step - loss: 0.0025\n",
            "Epoch 00033: early stopping\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 21s 151ms/step - loss: 0.0109\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 17s 148ms/step - loss: 0.0127\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0081\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 17s 150ms/step - loss: 0.0070\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 17s 150ms/step - loss: 0.0063\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0054\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 17s 151ms/step - loss: 0.0055\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 17s 150ms/step - loss: 0.0047\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 18s 152ms/step - loss: 0.0045\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 18s 151ms/step - loss: 0.0044\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 18s 152ms/step - loss: 0.0038\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0038\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 17s 150ms/step - loss: 0.0038\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 17s 150ms/step - loss: 0.0037\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 17s 150ms/step - loss: 0.0032\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0033\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 18s 152ms/step - loss: 0.0030\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0032\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 17s 150ms/step - loss: 0.0034\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 17s 149ms/step - loss: 0.0025\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 18s 151ms/step - loss: 0.0027\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 18s 151ms/step - loss: 0.0029\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 17s 150ms/step - loss: 0.0028\n",
            "Epoch 00023: early stopping\n",
            "[[0.81817348 0.80431857 0.81224003 0.81826523 0.81517617 0.8193357\n",
            "  0.80719354 0.80138243 0.7887815  0.78511133 0.7835515  0.78287864\n",
            "  0.77422315 0.77407022]\n",
            " [0.77033888 0.77682285 0.77926964 0.77807683 0.78749694 0.79055542\n",
            "  0.79153413 0.78511133 0.78339858 0.78605946 0.76975777 0.7785356\n",
            "  0.77590531 0.7761194 ]\n",
            " [0.78859799 0.79235992 0.78966846 0.80624541 0.81597137 0.82101786\n",
            "  0.83297651 0.8197333  0.82321997 0.83306827 0.83591265 0.83566797\n",
            "  0.84120382 0.83487277]\n",
            " [0.84618914 0.84600563 0.83646318 0.90469782 0.90488133 0.90139467\n",
            "  0.922437   0.91665647 0.89616467 0.90564595 0.91512723 0.93720944\n",
            "  0.96846709 0.96231955]\n",
            " [0.95568265 0.95504037 0.94925985 0.95051382 0.97972229 0.95794593\n",
            "  0.94347932 0.9465378  0.91683998 0.91723758 0.88998654 0.91246636\n",
            "  0.92020431 0.91534133]\n",
            " [0.92191705 0.92307928 0.93595547 0.92671886 0.92837044 0.90469782\n",
            "  0.90784805 0.91206875 0.91188525 0.90561537 0.91008074 0.89913139\n",
            "  0.90781747 0.9240274 ]\n",
            " [0.92586249 0.93262173 0.93571079 0.93161243 0.92286518 0.88729508\n",
            "  0.88821263 0.88815146 0.88087228 0.89283093 0.88857964 0.89702104\n",
            "  0.90754221 0.90613531]\n",
            " [0.9176046  0.93149009 0.9652251  0.96375703 0.99528994 0.99057989\n",
            "  0.9979814  0.98275018 0.96892586 0.9668461  0.97678615 0.97755077\n",
            "  0.96546978 0.98115977]\n",
            " [0.98067042 0.98189381 0.98467702 1.00568877 1.02923905 1.03092121\n",
            "  1.033368   1.03018718 1.03468314 1.00850257 0.99862368 1.0134573\n",
            "  1.02960607 1.01058233]\n",
            " [1.01926841 1.01822853 1.01620993 1.0239173  1.01143871 1.0323587\n",
            "  1.02367262 1.02960607 1.022908   1.03379618 1.00507707 1.02578297\n",
            "  1.02709812 1.01465011]\n",
            " [1.01740274 1.02125642 1.05410448 1.04951676 1.05018963 1.03541718\n",
            "  1.03856741 1.03327624 1.01422192 0.99204796 0.95442868 0.92038782\n",
            "  0.92005138 0.89414607]\n",
            " [0.9284622  0.93522143 0.9292574  0.93164301 0.96167727 0.98580866\n",
            "  0.97880475 0.98241375 0.97103621 0.98669562 1.00443479 1.01645461\n",
            "  1.01816736 1.01281502]\n",
            " [1.00143748 1.01113286 1.00911426 1.02348911 1.01201982 1.01244801\n",
            "  1.03471373 1.02856619 1.03703817 1.02238806 1.02593589 1.02501835\n",
            "  1.01663812 1.01529239]\n",
            " [0.99865427 0.99480059 0.99629924 0.98791901 1.00492415 1.0200942\n",
            "  1.00015292 0.96589797 0.9648275  0.97519574 0.96907879 0.96017862\n",
            "  0.95748715 0.95011622]\n",
            " [0.95088084 0.94195009 0.94730242 0.95794593 0.96048446 0.95552973\n",
            "  0.96369586 0.99030462 0.98164913 0.9678554  0.96525569 0.95436751\n",
            "  0.95944458 0.95965867]\n",
            " [0.98011989 0.97822364 0.98534989 0.97482873 0.9873379  0.98348422\n",
            "  0.99134451 0.99730854 0.98076217 0.98801077 1.00721801 1.02358087\n",
            "  1.02884145 1.03804747]\n",
            " [1.03012601 1.03287864 1.02403964 1.02578297 1.04110595 1.05349278\n",
            "  1.04752875 1.04661121 1.0408001  1.0393932  1.01672988 0.9845241\n",
            "  0.97773428 0.98525814]\n",
            " [0.97877416 0.97091387 0.98421825 0.94922926 0.92558723 0.91200759\n",
            "  0.89270859 0.95745657 0.9376988  0.93194886 0.91858331 0.91344507\n",
            "  0.93136775 0.9210301 ]\n",
            " [0.92509787 0.92809518 0.92020431 0.93188769 0.93775997 0.92047957\n",
            "  0.91451554 0.92109126 0.90417788 0.89754098 0.8852459  0.86674211\n",
            "  0.8587289  0.87588696]\n",
            " [0.88533766 0.9077563  0.90234279 0.88081111 0.87772205 0.87276731\n",
            "  0.87496942 0.86946415 0.8567103  0.8595241  0.85392709 0.84383411\n",
            "  0.86184854 0.87276731]\n",
            " [0.85759726 0.87126866 0.87181918 0.86450942 0.86319427 0.85976878\n",
            "  0.8545082  0.8615427  0.88604111 0.89243333 0.88420602 0.88289087\n",
            "  0.87695743 0.87919011]\n",
            " [0.87717152 0.8806276  0.86701737 0.87570345 0.89255566 0.8742048\n",
            "  0.86236849 0.85622094 0.85362124 0.85065451 0.84514925 0.83548446\n",
            "  0.81878517 0.80939564]\n",
            " [0.78520308 0.78468314 0.77838268 0.76238684 0.74388304 0.73238317\n",
            "  0.72531808 0.73697088 0.73088451 0.73067042 0.72302422 0.72265721\n",
            "  0.75030585 0.73323954]\n",
            " [0.72183142 0.70776242 0.72666381 0.72696966 0.72614387 0.72926352\n",
            "  0.72651089 0.73583925 0.72131148 0.71819183 0.71715195 0.70745657\n",
            "  0.70173722 0.69858698]\n",
            " [0.7136347  0.72641913 0.75214093 0.72229019 0.7116161  0.74131392\n",
            "  0.72923293 0.73700147 0.74801199 0.77177636 0.76198923 0.76327379\n",
            "  0.76033766 0.75394544]\n",
            " [0.74749205 0.73993761 0.69662956 0.70978101 0.71183019 0.70130903\n",
            "  0.7126254  0.72351358 0.82425985 0.82068143 0.85313188 0.86065574\n",
            "  0.84157083 0.85744434]\n",
            " [0.85842305 0.86438708 0.85065451 0.85392709 0.86288843 0.86145094\n",
            "  0.88252386 0.8669562  0.85316247 0.87811965 0.87536702 0.89671519\n",
            "  0.89998777 0.92476144]\n",
            " [0.92197822 0.92540372 0.93228529 0.91488255 0.90867384 0.91512723\n",
            "  0.92411916 0.93840225 0.8932897  0.86873012 0.87448006 0.88325789\n",
            "  0.85667972 0.85111329]\n",
            " [0.87252263 0.90179227 0.89231099 0.88221801 0.8782114  0.86346954\n",
            "  0.85603744 0.87827257 0.88949719 0.87671275 0.87126866 0.88426719\n",
            "  0.84517984 0.8563127 ]\n",
            " [0.8261561  0.82373991 0.81835699 0.81701126 0.79936384 0.84120382\n",
            "  0.85340714 0.87191094 0.86353071 0.7930022  0.79162589 0.76645461\n",
            "  0.76724982 0.75825789]\n",
            " [0.74727795 0.74773673 0.7443724  0.7626621  0.78156349 0.80957915\n",
            "  0.8235564  0.80710179 0.81618547 0.81725593 0.80174945 0.81306582\n",
            "  0.82328114 0.84753487]\n",
            " [0.84010276 0.84025569 0.82086494 0.83747247 0.81263763 0.80199413\n",
            "  0.80569489 0.80067898 0.81710301 0.8303768  0.84952288 0.84472107\n",
            "  0.82159897 0.79263518]\n",
            " [0.80441033 0.78529484 0.79817103 0.8301933  0.83976633 0.83780891\n",
            "  0.84046978 0.84001101 0.86609983 0.86203205 0.83316002 0.80722413\n",
            "  0.82664546 0.81428921]]\n",
            "[0.8520993590354919, 0.8361783862113953, 0.8446847438812256, 0.830044960975647, 0.8292667984962463, 0.8538230538368226, 0.8406580209732055, 0.8323784589767456, 0.8371436595916748, 0.8051584005355835, 0.820971405506134, 0.829833722114563, 0.8254167675971985, 0.8148316860198974, 0.831177294254303, 0.8173547506332397, 0.827463686466217, 0.8199903845787049, 0.8124163389205933, 0.82804434299469, 0.8207679390907288, 0.8174267649650574, 0.8238544344902039, 0.8034894227981567, 0.8160196661949157, 0.8127399325370789, 0.8163970112800598, 0.801377785205841, 0.813770079612732, 0.7992911338806152, 0.8105617642402649, 0.8099911570549011, 0.8017197370529174, 0.814176595211029, 0.806200110912323, 0.8059824109077454, 0.8091645121574402, 0.7875824451446534, 0.8029000520706177, 0.7978317618370057, 0.800793194770813, 0.7825689315795898, 0.8663051843643188, 0.8512205123901367, 0.8591852784156799, 0.8466959595680237, 0.8412082195281982, 0.8646530389785767, 0.856786572933197, 0.8457485318183899, 0.8526290774345398, 0.8215427041053772, 0.8413844466209411, 0.8422703146934509, 0.8412262797355652, 0.8229180216789246, 0.9604038715362548, 0.949190866947174, 0.9534066677093506, 0.9247081875801086, 0.9219292044639588, 0.9583744406700134, 0.9522130131721497, 0.9258652567863465, 0.9319853782653809, 0.8965305209159851, 0.922408378124237, 0.9277832746505738, 0.9246164083480835, 0.9091185569763184, 0.9557699799537659, 0.9504518270492553, 0.9573554515838623, 0.9389464020729065, 0.9310310006141662, 0.9503430247306823, 0.9541741251945496, 0.9372129678726197, 0.9329498410224915, 0.9161602020263672, 0.9386149168014526, 0.9314337611198426, 0.936810839176178, 0.9231905102729797, 0.9386824488639831, 0.9264291763305664, 0.9355482459068298, 0.9303156971931458, 0.9209803462028503, 0.9326444149017334, 0.9323960423469544, 0.9227154016494751, 0.9273443698883057, 0.9039215564727783, 0.9196120619773864, 0.9114346027374267, 0.9201377153396606, 0.8966179728507996, 0.9312432527542114, 0.9177540898323059, 0.9285887598991394, 0.9178418040275573, 0.9094575643539429, 0.9242785811424256, 0.9222769379615784, 0.9127955198287964, 0.9208969593048095, 0.8963552594184876, 0.9088624477386474, 0.9033393502235413, 0.9088599443435669, 0.8906163930892944, 1.0032474875450135, 0.9883557438850403, 0.9939610242843628, 0.9740955710411072, 0.9749092698097229, 0.9925583124160766, 0.9904419541358948, 0.9741637349128723, 0.981939697265625, 0.9435250878334045, 0.9660818576812744, 0.9730204701423645, 0.9756671905517578, 0.9480399847030639, 1.04391930103302, 1.036079478263855, 1.0423492908477783, 1.0198281288146973, 1.0142667531967162, 1.0365049362182617, 1.0376075029373169, 1.017413318157196, 1.0164498209953308, 0.9898109555244445, 1.0162397861480712, 1.0097695589065552, 1.0171738982200622, 0.9954638838768005, 1.0476032018661499, 1.0399773359298705, 1.0479222774505614, 1.0324301719665527, 1.0226035118103027, 1.0387080192565918, 1.0438172340393066, 1.0245638132095336, 1.025886106491089, 0.9999194025993348, 1.0230059266090392, 1.013955819606781, 1.0259946823120116, 1.0010469675064086, 0.9887732982635498, 0.9815879583358764, 0.9892005801200867, 0.9837068676948547, 0.9778943657875061, 0.9796079516410827, 0.9805064678192139, 0.9792657136917114, 0.9809772133827209, 0.9587868928909302, 0.9762032270431519, 0.9697534561157226, 0.9789839267730713, 0.9529157996177673, 1.0169614911079408, 1.0048094987869263, 1.015457558631897, 1.0082989931106567, 0.9973193287849427, 1.011538279056549, 1.0120691299438476, 0.9940061330795288, 1.0021108269691468, 0.9708384037017822, 0.9921357035636902, 0.979788863658905, 0.993295419216156, 0.9639451146125794, 1.0505311727523803, 1.0351526498794557, 1.0477847814559937, 1.024442195892334, 1.0197089076042176, 1.0405938386917115, 1.0388468742370605, 1.023396074771881, 1.025066351890564, 0.9963200449943542, 1.0181686997413635, 1.0132344245910645, 1.024096667766571, 1.0016450762748719, 0.9960435032844543, 0.9860721468925476, 0.9979747772216797, 0.9894225239753723, 0.9827302455902099, 0.9874670147895813, 0.9895248651504517, 0.9810226678848266, 0.9840251326560974, 0.9639876484870911, 0.9788168549537659, 0.9684483766555786, 0.9823567271232605, 0.9585572004318237, 0.9883243680000305, 0.9732666850090027, 0.9863969922065735, 0.9789937496185303, 0.9704951047897339, 0.9811508178710937, 0.9782070279121399, 0.970156466960907, 0.9753398180007935, 0.9491840243339539, 0.9666723251342774, 0.9600831747055054, 0.9734678506851197, 0.9428935170173645, 1.0333586931228638, 1.019946813583374, 1.031699275970459, 1.0101134061813355, 1.0043426513671876, 1.0265594005584717, 1.0244728922843933, 1.0047471523284912, 1.0129713892936707, 0.9791173934936523, 0.9984420895576477, 0.9977420926094055, 1.0024213671684266, 0.9833380341529846, 1.0403809309005738, 1.0268437147140503, 1.037908172607422, 1.0201584458351136, 1.014894151687622, 1.0283185720443726, 1.0303073406219483, 1.019142985343933, 1.0214271664619445, 0.9935175657272339, 1.0143860936164857, 1.0103344202041626, 1.0197004079818726, 0.9920575499534607, 0.94918133020401, 0.9407094240188598, 0.9560311913490296, 0.9562456130981445, 0.9378857612609863, 0.9448657631874084, 0.9435193300247192, 0.9422165751457214, 0.9455244660377502, 0.9327793955802918, 0.9404549241065979, 0.9258665323257447, 0.9365064263343811, 0.9209899544715882, 0.9166212677955627, 0.9007273316383362, 0.9134907364845276, 0.9116621017456055, 0.9048721790313721, 0.9100390434265136, 0.9053214073181153, 0.9062792062759399, 0.9184833765029907, 0.8882178068161011, 0.8996993660926819, 0.8986399054527283, 0.9029747724533081, 0.8773508310317993, 0.8931200981140137, 0.8807841300964355, 0.8921780467033387, 0.8885322451591492, 0.8759682178497314, 0.8879831194877624, 0.8858942270278931, 0.8818405628204345, 0.8861896991729736, 0.8669129371643066, 0.8792532205581665, 0.870271372795105, 0.8741791248321533, 0.8578693389892578, 0.9087400436401367, 0.8932342886924743, 0.9056026935577393, 0.90013507604599, 0.8883531212806701, 0.9029386878013611, 0.9012452840805054, 0.8930267453193664, 0.9005907416343689, 0.8739583611488342, 0.8885022401809692, 0.8818819522857666, 0.8891457200050354, 0.8661493539810181, 0.8751322984695434, 0.861117959022522, 0.8718563914299011, 0.8661845326423645, 0.8609237670898438, 0.8698040962219238, 0.8640933394432068, 0.8634236454963684, 0.8704479694366455, 0.8470359086990357, 0.8595344424247742, 0.8584619641304017, 0.8629042387008667, 0.8419679284095765, 0.7714026808738709, 0.7593184590339661, 0.7727608799934387, 0.7793354988098145, 0.7668146133422852, 0.7711159229278565, 0.7644139409065247, 0.7699732661247254, 0.772676694393158, 0.7675517797470093, 0.7734011292457581, 0.75708087682724, 0.7665049076080322, 0.7519261837005615, 0.7522411108016968, 0.7352117896080017, 0.7457044839859008, 0.7491607546806336, 0.7452979326248169, 0.75349200963974, 0.7405219912528992, 0.745117461681366, 0.7563861846923828, 0.7306798100471497, 0.7415291428565979, 0.7428407073020935, 0.7443552136421203, 0.7222695112228393, 0.7880305647850037, 0.7709692120552063, 0.783433985710144, 0.7773848176002502, 0.7708205223083496, 0.7915930390357971, 0.7779489994049072, 0.7769076466560364, 0.7814308404922485, 0.7548425197601318, 0.7686795473098755, 0.7693879723548889, 0.7654363870620727, 0.7567105412483215, 0.8422495007514954, 0.8156205654144287, 0.8444595456123352, 0.8338316082954407, 0.8173420190811157, 0.8458870053291321, 0.8358598351478577, 0.8271747469902039, 0.8334869384765625, 0.8086544513702393, 0.81863853931427, 0.8115884065628052, 0.8192102313041687, 0.807061779499054, 0.9190835952758789, 0.9009594202041626, 0.913935124874115, 0.8816831946372986, 0.8841580510139465, 0.9150169014930725, 0.9032850384712219, 0.8892173528671264, 0.897011935710907, 0.8626053690910339, 0.8782237410545349, 0.8975682616233825, 0.8876373529434204, 0.8799039006233216, 0.9194700360298157, 0.9094362497329712, 0.9160961747169495, 0.9016184091567994, 0.8963096737861633, 0.9117108821868897, 0.9086545586585999, 0.9014232516288757, 0.9056648254394531, 0.8810619831085205, 0.8984422564506531, 0.9010691523551941, 0.9029903531074523, 0.8828272461891175, 0.894447124004364, 0.8816868662834167, 0.8937501668930053, 0.8946072101593018, 0.8810193657875061, 0.8906922221183777, 0.888830304145813, 0.8882901906967163, 0.8929769635200501, 0.86943039894104, 0.8808791279792786, 0.8738487839698792, 0.8795346975326538, 0.8607690572738648, 0.8316777944564819, 0.8191851615905762, 0.8299123167991638, 0.8336572527885437, 0.8247116684913636, 0.8292829871177674, 0.8208153367042541, 0.829660165309906, 0.8336248397827148, 0.8169056296348571, 0.8287531018257142, 0.8236969947814942, 0.8281584262847901, 0.8052320718765259, 0.8429614663124084, 0.8339118957519531, 0.8422357559204101, 0.8423724412918091, 0.828551423549652, 0.8474656701087951, 0.8427707314491272, 0.8321860074996948, 0.8413073420524597, 0.8126659393310547, 0.8316262125968933, 0.8248660445213318, 0.8251728296279908, 0.8060417294502258, 0.8593842387199402, 0.8462694644927978, 0.8569425702095032, 0.8450180172920227, 0.8355893135070801, 0.8591566801071167, 0.8519192814826966, 0.8422582149505615, 0.8468457221984863, 0.8239595174789429, 0.8398764252662658, 0.8346174120903015, 0.8386435985565186, 0.8244061708450318]\n",
            "X train:  (3674, 60)\n",
            "y train:  (3674, 7)\n",
            "X test:  (63, 60)\n",
            "y test:  (63, 7)\n",
            "Epoch 1/200\n",
            "115/115 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Epoch 2/200\n",
            "115/115 [==============================] - 0s 2ms/step - loss: 0.0044\n",
            "Epoch 3/200\n",
            "115/115 [==============================] - 0s 3ms/step - loss: 0.0052\n",
            "Epoch 00003: early stopping\n",
            "X train:  (3674, 60, 1)\n",
            "y train:  (3674, 7)\n",
            "X train:  (3674, 60, 1)\n",
            "y train:  (3674, 7)\n",
            "Model: \"sequential_353\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_160 (Conv1D)          (None, 56, 64)            384       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_89 (MaxPooling (None, 28, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten_160 (Flatten)        (None, 1792)              0         \n",
            "_________________________________________________________________\n",
            "dense_533 (Dense)            (None, 7)                 12551     \n",
            "=================================================================\n",
            "Total params: 12,935\n",
            "Trainable params: 12,935\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "115/115 [==============================] - 1s 6ms/step - loss: 0.0038\n",
            "Epoch 2/200\n",
            "115/115 [==============================] - 1s 6ms/step - loss: 0.0084\n",
            "Epoch 3/200\n",
            "115/115 [==============================] - 1s 6ms/step - loss: 0.0046\n",
            "Epoch 4/200\n",
            "115/115 [==============================] - 1s 6ms/step - loss: 0.0038\n",
            "Epoch 5/200\n",
            "115/115 [==============================] - 1s 6ms/step - loss: 0.0029\n",
            "Epoch 6/200\n",
            "115/115 [==============================] - 1s 6ms/step - loss: 0.0029\n",
            "Epoch 7/200\n",
            "115/115 [==============================] - 1s 6ms/step - loss: 0.0025\n",
            "Epoch 8/200\n",
            "115/115 [==============================] - 1s 6ms/step - loss: 0.0024\n",
            "Epoch 9/200\n",
            "115/115 [==============================] - 1s 6ms/step - loss: 0.0021\n",
            "Epoch 10/200\n",
            "115/115 [==============================] - 1s 6ms/step - loss: 0.0022\n",
            "Epoch 11/200\n",
            "115/115 [==============================] - 1s 6ms/step - loss: 0.0019\n",
            "Epoch 12/200\n",
            "115/115 [==============================] - 1s 7ms/step - loss: 0.0020\n",
            "Epoch 13/200\n",
            "115/115 [==============================] - 1s 6ms/step - loss: 0.0019\n",
            "Epoch 14/200\n",
            "115/115 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 15/200\n",
            "115/115 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 16/200\n",
            "115/115 [==============================] - 1s 7ms/step - loss: 0.0020\n",
            "Epoch 17/200\n",
            "115/115 [==============================] - 1s 6ms/step - loss: 0.0020\n",
            "Epoch 00017: early stopping\n",
            "Model: \"sequential_354\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_178 (LSTM)              (None, 60, 256)           264192    \n",
            "_________________________________________________________________\n",
            "dropout_348 (Dropout)        (None, 60, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_179 (LSTM)              (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dropout_349 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_534 (Dense)            (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 462,215\n",
            "Trainable params: 462,215\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "115/115 [==============================] - 45s 369ms/step - loss: 0.0113\n",
            "Epoch 2/200\n",
            "115/115 [==============================] - 43s 371ms/step - loss: 0.0077\n",
            "Epoch 3/200\n",
            "115/115 [==============================] - 43s 378ms/step - loss: 0.0075\n",
            "Epoch 4/200\n",
            "115/115 [==============================] - 43s 373ms/step - loss: 0.0053\n",
            "Epoch 5/200\n",
            "115/115 [==============================] - 43s 375ms/step - loss: 0.0051\n",
            "Epoch 6/200\n",
            "115/115 [==============================] - 43s 375ms/step - loss: 0.0049\n",
            "Epoch 7/200\n",
            "115/115 [==============================] - 42s 369ms/step - loss: 0.0045\n",
            "Epoch 8/200\n",
            "115/115 [==============================] - 42s 367ms/step - loss: 0.0048\n",
            "Epoch 9/200\n",
            "115/115 [==============================] - 43s 371ms/step - loss: 0.0040\n",
            "Epoch 10/200\n",
            " 86/115 [=====================>........] - ETA: 10s - loss: 0.0043"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OiTP8uUyT_X"
      },
      "source": [
        ""
      ],
      "id": "2OiTP8uUyT_X",
      "execution_count": null,
      "outputs": []
    }
  ]
}